<!doctype html>
<html>
<head>
<meta charset='UTF-8'><meta name='viewport' content='width=device-width initial-scale=1'>

<link href='https://fonts.loli.net/css?family=Open+Sans:400italic,700italic,700,400&subset=latin,latin-ext' rel='stylesheet' type='text/css' /><style type='text/css'>html {overflow-x: initial !important;}:root { --bg-color:#ffffff; --text-color:#333333; --select-text-bg-color:#B5D6FC; --select-text-font-color:auto; --monospace:"Lucida Console",Consolas,"Courier",monospace; --title-bar-height:20px; }
.mac-os-11 { --title-bar-height:28px; }
html { font-size: 14px; background-color: var(--bg-color); color: var(--text-color); font-family: "Helvetica Neue", Helvetica, Arial, sans-serif; -webkit-font-smoothing: antialiased; }
body { margin: 0px; padding: 0px; height: auto; inset: 0px; font-size: 1rem; line-height: 1.42857; overflow-x: hidden; background: inherit; tab-size: 4; }
iframe { margin: auto; }
a.url { word-break: break-all; }
a:active, a:hover { outline: 0px; }
.in-text-selection, ::selection { text-shadow: none; background: var(--select-text-bg-color); color: var(--select-text-font-color); }
#write { margin: 0px auto; height: auto; width: inherit; word-break: normal; overflow-wrap: break-word; position: relative; white-space: normal; overflow-x: visible; padding-top: 36px; }
#write.first-line-indent p { text-indent: 2em; }
#write.first-line-indent li p, #write.first-line-indent p * { text-indent: 0px; }
#write.first-line-indent li { margin-left: 2em; }
.for-image #write { padding-left: 8px; padding-right: 8px; }
body.typora-export { padding-left: 30px; padding-right: 30px; }
.typora-export .footnote-line, .typora-export li, .typora-export p { white-space: pre-wrap; }
.typora-export .task-list-item input { pointer-events: none; }
@media screen and (max-width: 500px) {
  body.typora-export { padding-left: 0px; padding-right: 0px; }
  #write { padding-left: 20px; padding-right: 20px; }
  .CodeMirror-sizer { margin-left: 0px !important; }
  .CodeMirror-gutters { display: none !important; }
}
#write li > figure:last-child { margin-bottom: 0.5rem; }
#write ol, #write ul { position: relative; }
img { max-width: 100%; vertical-align: middle; image-orientation: from-image; }
button, input, select, textarea { color: inherit; font: inherit; }
input[type="checkbox"], input[type="radio"] { line-height: normal; padding: 0px; }
*, ::after, ::before { box-sizing: border-box; }
#write h1, #write h2, #write h3, #write h4, #write h5, #write h6, #write p, #write pre { width: inherit; }
#write h1, #write h2, #write h3, #write h4, #write h5, #write h6, #write p { position: relative; }
p { line-height: inherit; }
h1, h2, h3, h4, h5, h6 { break-after: avoid-page; break-inside: avoid; orphans: 4; }
p { orphans: 4; }
h1 { font-size: 2rem; }
h2 { font-size: 1.8rem; }
h3 { font-size: 1.6rem; }
h4 { font-size: 1.4rem; }
h5 { font-size: 1.2rem; }
h6 { font-size: 1rem; }
.md-math-block, .md-rawblock, h1, h2, h3, h4, h5, h6, p { margin-top: 1rem; margin-bottom: 1rem; }
.hidden { display: none; }
.md-blockmeta { color: rgb(204, 204, 204); font-weight: 700; font-style: italic; }
a { cursor: pointer; }
sup.md-footnote { padding: 2px 4px; background-color: rgba(238, 238, 238, 0.7); color: rgb(85, 85, 85); border-radius: 4px; cursor: pointer; }
sup.md-footnote a, sup.md-footnote a:hover { color: inherit; text-transform: inherit; text-decoration: inherit; }
#write input[type="checkbox"] { cursor: pointer; width: inherit; height: inherit; }
figure { overflow-x: auto; margin: 1.2em 0px; max-width: calc(100% + 16px); padding: 0px; }
figure > table { margin: 0px; }
tr { break-inside: avoid; break-after: auto; }
thead { display: table-header-group; }
table { border-collapse: collapse; border-spacing: 0px; width: 100%; overflow: auto; break-inside: auto; text-align: left; }
table.md-table td { min-width: 32px; }
.CodeMirror-gutters { border-right: 0px; background-color: inherit; }
.CodeMirror-linenumber { user-select: none; }
.CodeMirror { text-align: left; }
.CodeMirror-placeholder { opacity: 0.3; }
.CodeMirror pre { padding: 0px 4px; }
.CodeMirror-lines { padding: 0px; }
div.hr:focus { cursor: none; }
#write pre { white-space: pre-wrap; }
#write.fences-no-line-wrapping pre { white-space: pre; }
#write pre.ty-contain-cm { white-space: normal; }
.CodeMirror-gutters { margin-right: 4px; }
.md-fences { font-size: 0.9rem; display: block; break-inside: avoid; text-align: left; overflow: visible; white-space: pre; background: inherit; position: relative !important; }
.md-fences-adv-panel { width: 100%; margin-top: 10px; text-align: center; padding-top: 0px; padding-bottom: 8px; overflow-x: auto; }
#write .md-fences.mock-cm { white-space: pre-wrap; }
.md-fences.md-fences-with-lineno { padding-left: 0px; }
#write.fences-no-line-wrapping .md-fences.mock-cm { white-space: pre; overflow-x: auto; }
.md-fences.mock-cm.md-fences-with-lineno { padding-left: 8px; }
.CodeMirror-line, twitterwidget { break-inside: avoid; }
.footnotes { opacity: 0.8; font-size: 0.9rem; margin-top: 1em; margin-bottom: 1em; }
.footnotes + .footnotes { margin-top: 0px; }
.md-reset { margin: 0px; padding: 0px; border: 0px; outline: 0px; vertical-align: top; background: 0px 0px; text-decoration: none; text-shadow: none; float: none; position: static; width: auto; height: auto; white-space: nowrap; cursor: inherit; -webkit-tap-highlight-color: transparent; line-height: normal; font-weight: 400; text-align: left; box-sizing: content-box; direction: ltr; }
li div { padding-top: 0px; }
blockquote { margin: 1rem 0px; }
li .mathjax-block, li p { margin: 0.5rem 0px; }
li blockquote { margin: 1rem 0px; }
li { margin: 0px; position: relative; }
blockquote > :last-child { margin-bottom: 0px; }
blockquote > :first-child, li > :first-child { margin-top: 0px; }
.footnotes-area { color: rgb(136, 136, 136); margin-top: 0.714rem; padding-bottom: 0.143rem; white-space: normal; }
#write .footnote-line { white-space: pre-wrap; }
@media print {
  body, html { border: 1px solid transparent; height: 99%; break-after: avoid; break-before: avoid; font-variant-ligatures: no-common-ligatures; }
  #write { margin-top: 0px; padding-top: 0px; border-color: transparent !important; }
  .typora-export * { -webkit-print-color-adjust: exact; }
  .typora-export #write { break-after: avoid; }
  .typora-export #write::after { height: 0px; }
  .is-mac table { break-inside: avoid; }
  .typora-export-show-outline .typora-export-sidebar { display: none; }
}
.footnote-line { margin-top: 0.714em; font-size: 0.7em; }
a img, img a { cursor: pointer; }
pre.md-meta-block { font-size: 0.8rem; min-height: 0.8rem; white-space: pre-wrap; background: rgb(204, 204, 204); display: block; overflow-x: hidden; }
p > .md-image:only-child:not(.md-img-error) img, p > img:only-child { display: block; margin: auto; }
#write.first-line-indent p > .md-image:only-child:not(.md-img-error) img { left: -2em; position: relative; }
p > .md-image:only-child { display: inline-block; width: 100%; }
#write .MathJax_Display { margin: 0.8em 0px 0px; }
.md-math-block { width: 100%; }
.md-math-block:not(:empty)::after { display: none; }
.MathJax_ref { fill: currentcolor; }
[contenteditable="true"]:active, [contenteditable="true"]:focus, [contenteditable="false"]:active, [contenteditable="false"]:focus { outline: 0px; box-shadow: none; }
.md-task-list-item { position: relative; list-style-type: none; }
.task-list-item.md-task-list-item { padding-left: 0px; }
.md-task-list-item > input { position: absolute; top: 0px; left: 0px; margin-left: -1.2em; margin-top: calc(1em - 10px); border: none; }
.math { font-size: 1rem; }
.md-toc { min-height: 3.58rem; position: relative; font-size: 0.9rem; border-radius: 10px; }
.md-toc-content { position: relative; margin-left: 0px; }
.md-toc-content::after, .md-toc::after { display: none; }
.md-toc-item { display: block; color: rgb(65, 131, 196); }
.md-toc-item a { text-decoration: none; }
.md-toc-inner:hover { text-decoration: underline; }
.md-toc-inner { display: inline-block; cursor: pointer; }
.md-toc-h1 .md-toc-inner { margin-left: 0px; font-weight: 700; }
.md-toc-h2 .md-toc-inner { margin-left: 2em; }
.md-toc-h3 .md-toc-inner { margin-left: 4em; }
.md-toc-h4 .md-toc-inner { margin-left: 6em; }
.md-toc-h5 .md-toc-inner { margin-left: 8em; }
.md-toc-h6 .md-toc-inner { margin-left: 10em; }
@media screen and (max-width: 48em) {
  .md-toc-h3 .md-toc-inner { margin-left: 3.5em; }
  .md-toc-h4 .md-toc-inner { margin-left: 5em; }
  .md-toc-h5 .md-toc-inner { margin-left: 6.5em; }
  .md-toc-h6 .md-toc-inner { margin-left: 8em; }
}
a.md-toc-inner { font-size: inherit; font-style: inherit; font-weight: inherit; line-height: inherit; }
.footnote-line a:not(.reversefootnote) { color: inherit; }
.md-attr { display: none; }
.md-fn-count::after { content: "."; }
code, pre, samp, tt { font-family: var(--monospace); }
kbd { margin: 0px 0.1em; padding: 0.1em 0.6em; font-size: 0.8em; color: rgb(36, 39, 41); background: rgb(255, 255, 255); border: 1px solid rgb(173, 179, 185); border-radius: 3px; box-shadow: rgba(12, 13, 14, 0.2) 0px 1px 0px, rgb(255, 255, 255) 0px 0px 0px 2px inset; white-space: nowrap; vertical-align: middle; }
.md-comment { color: rgb(162, 127, 3); opacity: 0.8; font-family: var(--monospace); }
code { text-align: left; vertical-align: initial; }
a.md-print-anchor { white-space: pre !important; border-width: initial !important; border-style: none !important; border-color: initial !important; display: inline-block !important; position: absolute !important; width: 1px !important; right: 0px !important; outline: 0px !important; background: 0px 0px !important; text-decoration: initial !important; text-shadow: initial !important; }
.os-windows.monocolor-emoji .md-emoji { font-family: "Segoe UI Symbol", sans-serif; }
.md-diagram-panel > svg { max-width: 100%; }
[lang="flow"] svg, [lang="mermaid"] svg { max-width: 100%; height: auto; }
[lang="mermaid"] .node text { font-size: 1rem; }
table tr th { border-bottom: 0px; }
video { max-width: 100%; display: block; margin: 0px auto; }
iframe { max-width: 100%; width: 100%; border: none; }
.highlight td, .highlight tr { border: 0px; }
mark { background: rgb(255, 255, 0); color: rgb(0, 0, 0); }
.md-html-inline .md-plain, .md-html-inline strong, mark .md-inline-math, mark strong { color: inherit; }
.md-expand mark .md-meta { opacity: 0.3 !important; }
mark .md-meta { color: rgb(0, 0, 0); }
@media print {
  .typora-export h1, .typora-export h2, .typora-export h3, .typora-export h4, .typora-export h5, .typora-export h6 { break-inside: avoid; }
}
.md-diagram-panel .messageText { stroke: none !important; }
.md-diagram-panel .start-state { fill: var(--node-fill); }
.md-diagram-panel .edgeLabel rect { opacity: 1 !important; }
.md-require-zoom-fix { height: auto; margin-top: 16px; margin-bottom: 16px; }
.md-require-zoom-fix foreignobject { font-size: var(--mermaid-font-zoom); }
.md-fences.md-fences-math { font-size: 1em; }
.md-fences-advanced:not(.md-focus) { padding: 0px; white-space: nowrap; border: 0px; }
.md-fences-advanced:not(.md-focus) { background: inherit; }
.typora-export-show-outline .typora-export-content { max-width: 1440px; margin: auto; display: flex; flex-direction: row; }
.typora-export-sidebar { width: 300px; font-size: 0.8rem; margin-top: 80px; margin-right: 18px; }
.typora-export-show-outline #write { --webkit-flex:2; flex: 2 1 0%; }
.typora-export-sidebar .outline-content { position: fixed; top: 0px; max-height: 100%; overflow: hidden auto; padding-bottom: 30px; padding-top: 60px; width: 300px; }
@media screen and (max-width: 1024px) {
  .typora-export-sidebar, .typora-export-sidebar .outline-content { width: 240px; }
}
@media screen and (max-width: 800px) {
  .typora-export-sidebar { display: none; }
}
.outline-content li, .outline-content ul { margin-left: 0px; margin-right: 0px; padding-left: 0px; padding-right: 0px; list-style: none; }
.outline-content ul { margin-top: 0px; margin-bottom: 0px; }
.outline-content strong { font-weight: 400; }
.outline-expander { width: 1rem; height: 1.42857rem; position: relative; display: table-cell; vertical-align: middle; cursor: pointer; padding-left: 4px; }
.outline-expander::before { content: ""; position: relative; font-family: Ionicons; display: inline-block; font-size: 8px; vertical-align: middle; }
.outline-item { padding-top: 3px; padding-bottom: 3px; cursor: pointer; }
.outline-expander:hover::before { content: ""; }
.outline-h1 > .outline-item { padding-left: 0px; }
.outline-h2 > .outline-item { padding-left: 1em; }
.outline-h3 > .outline-item { padding-left: 2em; }
.outline-h4 > .outline-item { padding-left: 3em; }
.outline-h5 > .outline-item { padding-left: 4em; }
.outline-h6 > .outline-item { padding-left: 5em; }
.outline-label { cursor: pointer; display: table-cell; vertical-align: middle; text-decoration: none; color: inherit; }
.outline-label:hover { text-decoration: underline; }
.outline-item:hover { border-color: rgb(245, 245, 245); background-color: var(--item-hover-bg-color); }
.outline-item:hover { margin-left: -28px; margin-right: -28px; border-left: 28px solid transparent; border-right: 28px solid transparent; }
.outline-item-single .outline-expander::before, .outline-item-single .outline-expander:hover::before { display: none; }
.outline-item-open > .outline-item > .outline-expander::before { content: ""; }
.outline-children { display: none; }
.info-panel-tab-wrapper { display: none; }
.outline-item-open > .outline-children { display: block; }
.typora-export .outline-item { padding-top: 1px; padding-bottom: 1px; }
.typora-export .outline-item:hover { margin-right: -8px; border-right: 8px solid transparent; }
.typora-export .outline-expander::before { content: "+"; font-family: inherit; top: -1px; }
.typora-export .outline-expander:hover::before, .typora-export .outline-item-open > .outline-item > .outline-expander::before { content: "−"; }
.typora-export-collapse-outline .outline-children { display: none; }
.typora-export-collapse-outline .outline-item-open > .outline-children, .typora-export-no-collapse-outline .outline-children { display: block; }
.typora-export-no-collapse-outline .outline-expander::before { content: "" !important; }
.typora-export-show-outline .outline-item-active > .outline-item .outline-label { font-weight: 700; }
.md-inline-math-container mjx-container { zoom: 0.95; }


.CodeMirror { height: auto; }
.CodeMirror.cm-s-inner { background: inherit; }
.CodeMirror-scroll { overflow: auto hidden; z-index: 3; }
.CodeMirror-gutter-filler, .CodeMirror-scrollbar-filler { background-color: rgb(255, 255, 255); }
.CodeMirror-gutters { border-right: 1px solid rgb(221, 221, 221); background: inherit; white-space: nowrap; }
.CodeMirror-linenumber { padding: 0px 3px 0px 5px; text-align: right; color: rgb(153, 153, 153); }
.cm-s-inner .cm-keyword { color: rgb(119, 0, 136); }
.cm-s-inner .cm-atom, .cm-s-inner.cm-atom { color: rgb(34, 17, 153); }
.cm-s-inner .cm-number { color: rgb(17, 102, 68); }
.cm-s-inner .cm-def { color: rgb(0, 0, 255); }
.cm-s-inner .cm-variable { color: rgb(0, 0, 0); }
.cm-s-inner .cm-variable-2 { color: rgb(0, 85, 170); }
.cm-s-inner .cm-variable-3 { color: rgb(0, 136, 85); }
.cm-s-inner .cm-string { color: rgb(170, 17, 17); }
.cm-s-inner .cm-property { color: rgb(0, 0, 0); }
.cm-s-inner .cm-operator { color: rgb(152, 26, 26); }
.cm-s-inner .cm-comment, .cm-s-inner.cm-comment { color: rgb(170, 85, 0); }
.cm-s-inner .cm-string-2 { color: rgb(255, 85, 0); }
.cm-s-inner .cm-meta { color: rgb(85, 85, 85); }
.cm-s-inner .cm-qualifier { color: rgb(85, 85, 85); }
.cm-s-inner .cm-builtin { color: rgb(51, 0, 170); }
.cm-s-inner .cm-bracket { color: rgb(153, 153, 119); }
.cm-s-inner .cm-tag { color: rgb(17, 119, 0); }
.cm-s-inner .cm-attribute { color: rgb(0, 0, 204); }
.cm-s-inner .cm-header, .cm-s-inner.cm-header { color: rgb(0, 0, 255); }
.cm-s-inner .cm-quote, .cm-s-inner.cm-quote { color: rgb(0, 153, 0); }
.cm-s-inner .cm-hr, .cm-s-inner.cm-hr { color: rgb(153, 153, 153); }
.cm-s-inner .cm-link, .cm-s-inner.cm-link { color: rgb(0, 0, 204); }
.cm-negative { color: rgb(221, 68, 68); }
.cm-positive { color: rgb(34, 153, 34); }
.cm-header, .cm-strong { font-weight: 700; }
.cm-del { text-decoration: line-through; }
.cm-em { font-style: italic; }
.cm-link { text-decoration: underline; }
.cm-error { color: red; }
.cm-invalidchar { color: red; }
.cm-constant { color: rgb(38, 139, 210); }
.cm-defined { color: rgb(181, 137, 0); }
div.CodeMirror span.CodeMirror-matchingbracket { color: rgb(0, 255, 0); }
div.CodeMirror span.CodeMirror-nonmatchingbracket { color: rgb(255, 34, 34); }
.cm-s-inner .CodeMirror-activeline-background { background: inherit; }
.CodeMirror { position: relative; overflow: hidden; }
.CodeMirror-scroll { height: 100%; outline: 0px; position: relative; box-sizing: content-box; background: inherit; }
.CodeMirror-sizer { position: relative; }
.CodeMirror-gutter-filler, .CodeMirror-hscrollbar, .CodeMirror-scrollbar-filler, .CodeMirror-vscrollbar { position: absolute; z-index: 6; display: none; outline: 0px; }
.CodeMirror-vscrollbar { right: 0px; top: 0px; overflow: hidden; }
.CodeMirror-hscrollbar { bottom: 0px; left: 0px; overflow: auto hidden; }
.CodeMirror-scrollbar-filler { right: 0px; bottom: 0px; }
.CodeMirror-gutter-filler { left: 0px; bottom: 0px; }
.CodeMirror-gutters { position: absolute; left: 0px; top: 0px; padding-bottom: 10px; z-index: 3; overflow-y: hidden; }
.CodeMirror-gutter { white-space: normal; height: 100%; box-sizing: content-box; padding-bottom: 30px; margin-bottom: -32px; display: inline-block; }
.CodeMirror-gutter-wrapper { position: absolute; z-index: 4; background: 0px 0px !important; border: none !important; }
.CodeMirror-gutter-background { position: absolute; top: 0px; bottom: 0px; z-index: 4; }
.CodeMirror-gutter-elt { position: absolute; cursor: default; z-index: 4; }
.CodeMirror-lines { cursor: text; }
.CodeMirror pre { border-radius: 0px; border-width: 0px; background: 0px 0px; font-family: inherit; font-size: inherit; margin: 0px; white-space: pre; overflow-wrap: normal; color: inherit; z-index: 2; position: relative; overflow: visible; }
.CodeMirror-wrap pre { overflow-wrap: break-word; white-space: pre-wrap; word-break: normal; }
.CodeMirror-code pre { border-right: 30px solid transparent; width: fit-content; }
.CodeMirror-wrap .CodeMirror-code pre { border-right: none; width: auto; }
.CodeMirror-linebackground { position: absolute; inset: 0px; z-index: 0; }
.CodeMirror-linewidget { position: relative; z-index: 2; overflow: auto; }
.CodeMirror-wrap .CodeMirror-scroll { overflow-x: hidden; }
.CodeMirror-measure { position: absolute; width: 100%; height: 0px; overflow: hidden; visibility: hidden; }
.CodeMirror-measure pre { position: static; }
.CodeMirror div.CodeMirror-cursor { position: absolute; visibility: hidden; border-right: none; width: 0px; }
.CodeMirror div.CodeMirror-cursor { visibility: hidden; }
.CodeMirror-focused div.CodeMirror-cursor { visibility: inherit; }
.cm-searching { background: rgba(255, 255, 0, 0.4); }
span.cm-underlined { text-decoration: underline; }
span.cm-strikethrough { text-decoration: line-through; }
.cm-tw-syntaxerror { color: rgb(255, 255, 255); background-color: rgb(153, 0, 0); }
.cm-tw-deleted { text-decoration: line-through; }
.cm-tw-header5 { font-weight: 700; }
.cm-tw-listitem:first-child { padding-left: 10px; }
.cm-tw-box { border-style: solid; border-right-width: 1px; border-bottom-width: 1px; border-left-width: 1px; border-color: inherit; border-top-width: 0px !important; }
.cm-tw-underline { text-decoration: underline; }
@media print {
  .CodeMirror div.CodeMirror-cursor { visibility: hidden; }
}


:root {
    --side-bar-bg-color: #fafafa;
    --control-text-color: #777;
}

@include-when-export url(https://fonts.loli.net/css?family=Open+Sans:400italic,700italic,700,400&subset=latin,latin-ext);

/* open-sans-regular - latin-ext_latin */
  /* open-sans-italic - latin-ext_latin */
    /* open-sans-700 - latin-ext_latin */
    /* open-sans-700italic - latin-ext_latin */
  html {
    font-size: 16px;
    -webkit-font-smoothing: antialiased;
}

body {
    font-family: "Open Sans","Clear Sans", "Helvetica Neue", Helvetica, Arial, 'Segoe UI Emoji', sans-serif;
    color: rgb(51, 51, 51);
    line-height: 1.6;
}

#write {
    max-width: 860px;
  	margin: 0 auto;
  	padding: 30px;
    padding-bottom: 100px;
}

@media only screen and (min-width: 1400px) {
	#write {
		max-width: 1024px;
	}
}

@media only screen and (min-width: 1800px) {
	#write {
		max-width: 1200px;
	}
}

#write > ul:first-child,
#write > ol:first-child{
    margin-top: 30px;
}

a {
    color: #4183C4;
}
h1,
h2,
h3,
h4,
h5,
h6 {
    position: relative;
    margin-top: 1rem;
    margin-bottom: 1rem;
    font-weight: bold;
    line-height: 1.4;
    cursor: text;
}
h1:hover a.anchor,
h2:hover a.anchor,
h3:hover a.anchor,
h4:hover a.anchor,
h5:hover a.anchor,
h6:hover a.anchor {
    text-decoration: none;
}
h1 tt,
h1 code {
    font-size: inherit;
}
h2 tt,
h2 code {
    font-size: inherit;
}
h3 tt,
h3 code {
    font-size: inherit;
}
h4 tt,
h4 code {
    font-size: inherit;
}
h5 tt,
h5 code {
    font-size: inherit;
}
h6 tt,
h6 code {
    font-size: inherit;
}
h1 {
    font-size: 2.25em;
    line-height: 1.2;
    border-bottom: 1px solid #eee;
}
h2 {
    font-size: 1.75em;
    line-height: 1.225;
    border-bottom: 1px solid #eee;
}

/*@media print {
    .typora-export h1,
    .typora-export h2 {
        border-bottom: none;
        padding-bottom: initial;
    }

    .typora-export h1::after,
    .typora-export h2::after {
        content: "";
        display: block;
        height: 100px;
        margin-top: -96px;
        border-top: 1px solid #eee;
    }
}*/

h3 {
    font-size: 1.5em;
    line-height: 1.43;
}
h4 {
    font-size: 1.25em;
}
h5 {
    font-size: 1em;
}
h6 {
   font-size: 1em;
    color: #777;
}
p,
blockquote,
ul,
ol,
dl,
table{
    margin: 0.8em 0;
}
li>ol,
li>ul {
    margin: 0 0;
}
hr {
    height: 2px;
    padding: 0;
    margin: 16px 0;
    background-color: #e7e7e7;
    border: 0 none;
    overflow: hidden;
    box-sizing: content-box;
}

li p.first {
    display: inline-block;
}
ul,
ol {
    padding-left: 30px;
}
ul:first-child,
ol:first-child {
    margin-top: 0;
}
ul:last-child,
ol:last-child {
    margin-bottom: 0;
}
blockquote {
    border-left: 4px solid #dfe2e5;
    padding: 0 15px;
    color: #777777;
}
blockquote blockquote {
    padding-right: 0;
}
table {
    padding: 0;
    word-break: initial;
}
table tr {
    border: 1px solid #dfe2e5;
    margin: 0;
    padding: 0;
}
table tr:nth-child(2n),
thead {
    background-color: #f8f8f8;
}
table th {
    font-weight: bold;
    border: 1px solid #dfe2e5;
    border-bottom: 0;
    margin: 0;
    padding: 6px 13px;
}
table td {
    border: 1px solid #dfe2e5;
    margin: 0;
    padding: 6px 13px;
}
table th:first-child,
table td:first-child {
    margin-top: 0;
}
table th:last-child,
table td:last-child {
    margin-bottom: 0;
}

.CodeMirror-lines {
    padding-left: 4px;
}

.code-tooltip {
    box-shadow: 0 1px 1px 0 rgba(0,28,36,.3);
    border-top: 1px solid #eef2f2;
}

.md-fences,
code,
tt {
    border: 1px solid #e7eaed;
    background-color: #f8f8f8;
    border-radius: 3px;
    padding: 0;
    padding: 2px 4px 0px 4px;
    font-size: 0.9em;
}

code {
    background-color: #f3f4f4;
    padding: 0 2px 0 2px;
}

.md-fences {
    margin-bottom: 15px;
    margin-top: 15px;
    padding-top: 8px;
    padding-bottom: 6px;
}


.md-task-list-item > input {
  margin-left: -1.3em;
}

@media print {
    html {
        font-size: 13px;
    }
    table,
    pre {
        page-break-inside: avoid;
    }
    pre {
        word-wrap: break-word;
    }
}

.md-fences {
	background-color: #f8f8f8;
}
#write pre.md-meta-block {
	padding: 1rem;
    font-size: 85%;
    line-height: 1.45;
    background-color: #f7f7f7;
    border: 0;
    border-radius: 3px;
    color: #777777;
    margin-top: 0 !important;
}

.mathjax-block>.code-tooltip {
	bottom: .375rem;
}

.md-mathjax-midline {
    background: #fafafa;
}

#write>h3.md-focus:before{
	left: -1.5625rem;
	top: .375rem;
}
#write>h4.md-focus:before{
	left: -1.5625rem;
	top: .285714286rem;
}
#write>h5.md-focus:before{
	left: -1.5625rem;
	top: .285714286rem;
}
#write>h6.md-focus:before{
	left: -1.5625rem;
	top: .285714286rem;
}
.md-image>.md-meta {
    /*border: 1px solid #ddd;*/
    border-radius: 3px;
    padding: 2px 0px 0px 4px;
    font-size: 0.9em;
    color: inherit;
}

.md-tag {
    color: #a7a7a7;
    opacity: 1;
}

.md-toc { 
    margin-top:20px;
    padding-bottom:20px;
}

.sidebar-tabs {
    border-bottom: none;
}

#typora-quick-open {
    border: 1px solid #ddd;
    background-color: #f8f8f8;
}

#typora-quick-open-item {
    background-color: #FAFAFA;
    border-color: #FEFEFE #e5e5e5 #e5e5e5 #eee;
    border-style: solid;
    border-width: 1px;
}

/** focus mode */
.on-focus-mode blockquote {
    border-left-color: rgba(85, 85, 85, 0.12);
}

header, .context-menu, .megamenu-content, footer{
    font-family: "Segoe UI", "Arial", sans-serif;
}

.file-node-content:hover .file-node-icon,
.file-node-content:hover .file-node-open-state{
    visibility: visible;
}

.mac-seamless-mode #typora-sidebar {
    background-color: #fafafa;
    background-color: var(--side-bar-bg-color);
}

.md-lang {
    color: #b4654d;
}

/*.html-for-mac {
    --item-hover-bg-color: #E6F0FE;
}*/

#md-notification .btn {
    border: 0;
}

.dropdown-menu .divider {
    border-color: #e5e5e5;
    opacity: 0.4;
}

.ty-preferences .window-content {
    background-color: #fafafa;
}

.ty-preferences .nav-group-item.active {
    color: white;
    background: #999;
}

.menu-item-container a.menu-style-btn {
    background-color: #f5f8fa;
    background-image: linear-gradient( 180deg , hsla(0, 0%, 100%, 0.8), hsla(0, 0%, 100%, 0)); 
}



mjx-container[jax="SVG"] {
  direction: ltr;
}

mjx-container[jax="SVG"] > svg {
  overflow: visible;
  min-height: 1px;
  min-width: 1px;
}

mjx-container[jax="SVG"] > svg a {
  fill: blue;
  stroke: blue;
}

mjx-assistive-mml {
  position: absolute !important;
  top: 0px;
  left: 0px;
  clip: rect(1px, 1px, 1px, 1px);
  padding: 1px 0px 0px 0px !important;
  border: 0px !important;
  display: block !important;
  width: auto !important;
  overflow: hidden !important;
  -webkit-touch-callout: none;
  -webkit-user-select: none;
  -khtml-user-select: none;
  -moz-user-select: none;
  -ms-user-select: none;
  user-select: none;
}

mjx-assistive-mml[display="block"] {
  width: 100% !important;
}

mjx-container[jax="SVG"][display="true"] {
  display: block;
  text-align: center;
  margin: 1em 0;
}

mjx-container[jax="SVG"][display="true"][width="full"] {
  display: flex;
}

mjx-container[jax="SVG"][justify="left"] {
  text-align: left;
}

mjx-container[jax="SVG"][justify="right"] {
  text-align: right;
}

g[data-mml-node="merror"] > g {
  fill: red;
  stroke: red;
}

g[data-mml-node="merror"] > rect[data-background] {
  fill: yellow;
  stroke: none;
}

g[data-mml-node="mtable"] > line[data-line], svg[data-table] > g > line[data-line] {
  stroke-width: 70px;
  fill: none;
}

g[data-mml-node="mtable"] > rect[data-frame], svg[data-table] > g > rect[data-frame] {
  stroke-width: 70px;
  fill: none;
}

g[data-mml-node="mtable"] > .mjx-dashed, svg[data-table] > g > .mjx-dashed {
  stroke-dasharray: 140;
}

g[data-mml-node="mtable"] > .mjx-dotted, svg[data-table] > g > .mjx-dotted {
  stroke-linecap: round;
  stroke-dasharray: 0,140;
}

g[data-mml-node="mtable"] > g > svg {
  overflow: visible;
}

[jax="SVG"] mjx-tool {
  display: inline-block;
  position: relative;
  width: 0;
  height: 0;
}

[jax="SVG"] mjx-tool > mjx-tip {
  position: absolute;
  top: 0;
  left: 0;
}

mjx-tool > mjx-tip {
  display: inline-block;
  padding: .2em;
  border: 1px solid #888;
  font-size: 70%;
  background-color: #F8F8F8;
  color: black;
  box-shadow: 2px 2px 5px #AAAAAA;
}

g[data-mml-node="maction"][data-toggle] {
  cursor: pointer;
}

mjx-status {
  display: block;
  position: fixed;
  left: 1em;
  bottom: 1em;
  min-width: 25%;
  padding: .2em .4em;
  border: 1px solid #888;
  font-size: 90%;
  background-color: #F8F8F8;
  color: black;
}

foreignObject[data-mjx-xml] {
  font-family: initial;
  line-height: normal;
  overflow: visible;
}

mjx-container[jax="SVG"] path[data-c], mjx-container[jax="SVG"] use[data-c] {
  stroke-width: 3;
}

g[data-mml-node="xypic"] path {
  stroke-width: inherit;
}

.MathJax g[data-mml-node="xypic"] path {
  stroke-width: inherit;
}

</style><title>README</title>
</head>
<body class='typora-export os-windows'><div class='typora-export-content'>
<div id='write'  class=''><p><span>This is a </span><strong><a href='https://pytorch.org'><span>PyTorch</span></a><span> Tutorial to Object Detection</span></strong><span>.</span></p><p><span>This is the third in </span><a href='https://github.com/sgrvinod/Deep-Tutorials-for-PyTorch'><span>a series of tutorials</span></a><span> I&#39;m writing about </span><em><span>implementing</span></em><span> cool models on your own with the amazing PyTorch library.</span></p><p><span>Basic knowledge of PyTorch, convolutional neural networks is assumed.</span></p><p><span>If you&#39;re new to PyTorch, first read </span><a href='https://pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html'><span>Deep Learning with PyTorch: A 60 Minute Blitz</span></a><span> and </span><a href='https://pytorch.org/tutorials/beginner/pytorch_with_examples.html'><span>Learning PyTorch with Examples</span></a><span>.</span></p><p><span>Questions, suggestions, or corrections can be posted as issues.</span></p><p><span>I&#39;m using </span><code>PyTorch 0.4</code><span> in </span><code>Python 3.6</code><span>.</span></p><hr /><p><strong><span>27 Jan 2020</span></strong><span>: Working code for two new tutorials has been added — </span><a href='https://github.com/sgrvinod/a-PyTorch-Tutorial-to-Super-Resolution'><span>Super-Resolution</span></a><span> and </span><a href='https://github.com/sgrvinod/a-PyTorch-Tutorial-to-Machine-Translation'><span>Machine Translation</span></a></p><hr /><h1 id='contents'><span>Contents</span></h1><p><a href='https://github.com/sgrvinod/a-PyTorch-Tutorial-to-Object-Detection#objective'><strong><em><span>Objective</span></em></strong></a></p><p><a href='https://github.com/sgrvinod/a-PyTorch-Tutorial-to-Object-Detection#concepts'><strong><em><span>Concepts</span></em></strong></a></p><p><a href='https://github.com/sgrvinod/a-PyTorch-Tutorial-to-Object-Detection#overview'><strong><em><span>Overview</span></em></strong></a></p><p><a href='https://github.com/sgrvinod/a-PyTorch-Tutorial-to-Object-Detection#implementation'><strong><em><span>Implementation</span></em></strong></a></p><p><a href='https://github.com/sgrvinod/a-PyTorch-Tutorial-to-Object-Detection#training'><strong><em><span>Training</span></em></strong></a></p><p><a href='https://github.com/sgrvinod/a-PyTorch-Tutorial-to-Object-Detection#evaluation'><strong><em><span>Evaluation</span></em></strong></a></p><p><a href='https://github.com/sgrvinod/a-PyTorch-Tutorial-to-Object-Detection#inference'><strong><em><span>Inference</span></em></strong></a></p><p><a href='https://github.com/sgrvinod/a-PyTorch-Tutorial-to-Object-Detection#faqs'><strong><em><span>Frequently Asked Questions</span></em></strong></a></p><h1 id='objective'><span>Objective</span></h1><p><strong><span>To build a model that can detect and localize specific objects in images.</span></strong></p><p align="center">
<img src="./img/baseball.gif">
</p><p><span>We will be implementing the </span><a href='https://arxiv.org/abs/1512.02325'><span>Single Shot Multibox Detector (SSD)</span></a><span>, a popular, powerful, and especially nimble network for this task. The authors&#39; original implementation can be found </span><a href='https://github.com/weiliu89/caffe/tree/ssd'><span>here</span></a><span>.</span></p><p><span>Here are some examples of object detection in images not seen during training –</span></p><hr /><p align="center">
<img src="./img/000001.jpg">
</p><hr /><p align="center">
<img src="./img/000022.jpg">
</p><hr /><p align="center">
<img src="./img/000069.jpg">
</p><hr /><p align="center">
<img src="./img/000082.jpg">
</p><hr /><p align="center">
<img src="./img/000144.jpg">
</p><hr /><p align="center">
<img src="./img/000139.jpg">
</p><hr /><p align="center">
<img src="./img/000116.jpg">
</p><hr /><p align="center">
<img src="./img/000098.jpg">
</p><hr /><p><span>There are more examples at the </span><a href='https://github.com/sgrvinod/a-PyTorch-Tutorial-to-Object-Detection#some-more-examples'><span>end of the tutorial</span></a><span>.</span></p><hr /><h1 id='concepts'><span>Concepts</span></h1><ul><li><strong><span>Object Detection</span></strong><span>. duh.</span></li><li><strong><span>Single-Shot Detection</span></strong><span>. Earlier architectures for object detection consisted of two distinct stages – a region proposal network that performs object localization and a classifier for detecting the types of objects in the proposed regions. Computationally, these can be very expensive and therefore ill-suited for real-world, real-time applications. Single-shot models encapsulate both localization and detection tasks in a single forward sweep of the network, resulting in significantly faster detections while deployable on lighter hardware.</span></li><li><strong><span>Multiscale Feature Maps</span></strong><span>. In image classification tasks, we base our predictions on the final convolutional feature map – the smallest but deepest representation of the original image. In object detection, feature maps from intermediate convolutional layers can also be </span><em><span>directly</span></em><span> useful because they represent the original image at different scales. Therefore, a fixed-size filter operating on different feature maps will be able to detect objects of various sizes.</span></li><li><strong><span>Priors</span></strong><span>. These are pre-computed boxes defined at specific positions on specific feature maps, with specific aspect ratios and scales. They are carefully chosen to match the characteristics of objects&#39; bounding boxes (i.e. the ground truths) in the dataset.</span></li><li><strong><span>Multibox</span></strong><span>. This is </span><a href='https://arxiv.org/abs/1312.2249'><span>a technique</span></a><span> that formulates predicting an object&#39;s bounding box as a </span><em><span>regression</span></em><span> problem, wherein a detected object&#39;s coordinates are regressed to its ground truth&#39;s coordinates. In addition, for each predicted box, scores are generated for various object types. Priors serve as feasible starting points for predictions because they are modeled on the ground truths. Therefore, there will be as many predicted boxes as there are priors, most of whom will contain no object.</span></li><li><strong><span>Hard Negative Mining</span></strong><span>. This refers to explicitly choosing the most egregious false positives predicted by a model and forcing it to learn from these examples. In other words, we are mining only those negatives that the model found </span><em><span>hardest</span></em><span> to identify correctly. In the context of object detection, where the vast majority of predicted boxes do not contain an object, this also serves to reduce the negative-positive imbalance.</span></li><li><strong><span>Non-Maximum Suppression</span></strong><span>. At any given location, multiple priors can overlap significantly. Therefore, predictions arising out of these priors could actually be duplicates of the same object. Non-Maximum Suppression (NMS) is a means to remove redundant predictions by suppressing all but the one with the maximum score.</span></li></ul><h1 id='overview'><span>Overview</span></h1><p><span>In this section, I will present an overview of this model. If you&#39;re already familiar with it, you can skip straight to the </span><a href='https://github.com/sgrvinod/a-PyTorch-Tutorial-to-Object-Detection#implementation'><span>Implementation</span></a><span> section or the commented code.</span></p><p><span>As we proceed, you will notice that there&#39;s a fair bit of engineering that&#39;s resulted in the SSD&#39;s very specific structure and formulation. Don&#39;t worry if some aspects of it seem contrived or unspontaneous at first. Remember, it&#39;s built upon </span><em><span>years</span></em><span> of (often empirical) research in this field.</span></p><h3 id='some-definitions'><span>Some definitions</span></h3><p><span>A box is a box. A </span><em><span>bounding</span></em><span> box is a box that wraps around an object i.e. represents its bounds.</span></p><p><span>In this tutorial, we will encounter both types – just boxes and bounding boxes. But all boxes are represented on images and we need to be able to measure their positions, shapes, sizes, and other properties.</span></p><h4 id='boundary-coordinates'><span>Boundary coordinates</span></h4><p><span>The most obvious way to represent a box is by the pixel coordinates of the </span><code>x</code><span> and </span><code>y</code><span> lines that constitute its boundaries.</span></p><p><img src="./img/bc1.PNG" referrerpolicy="no-referrer"></p><p><span>The boundary coordinates of a box are simply </span><strong><code>(x_min, y_min, x_max, y_max)</code></strong><span>.</span></p><p><span>But pixel values are next to useless if we don&#39;t know the actual dimensions of the image.</span>
<span>A better way would be to represent all coordinates is in their </span><em><span>fractional</span></em><span> form.</span></p><p><img src="./img/bc2.PNG" referrerpolicy="no-referrer"></p><p><span>Now the coordinates are size-invariant and all boxes across all images are measured on the same scale.</span></p><h4 id='center-size-coordinates'><span>Center-Size coordinates</span></h4><p><span>This is a more explicit way of representing a box&#39;s position and dimensions.</span></p><p><img src="./img/cs.PNG" referrerpolicy="no-referrer"></p><p><span>The center-size coordinates of a box are </span><strong><code>(c_x, c_y, w, h)</code></strong><span>.</span></p><p><span>In the code, you will find that we routinely use both coordinate systems depending upon their suitability for the task, and </span><em><span>always</span></em><span> in their fractional forms.</span></p><h4 id='jaccard-index'><span>Jaccard Index</span></h4><p><span>The Jaccard Index or Jaccard Overlap or Intersection-over-Union (IoU) measure the </span><strong><span>degree or extent to which two boxes overlap</span></strong><span>.</span></p><p><img src="./img/jaccard.jpg" referrerpolicy="no-referrer"></p><p><span>An IoU of </span><code>1</code><span> implies they are the </span><em><span>same</span></em><span> box, while a value of </span><code>0</code><span> indicates they&#39;re mutually exclusive spaces.</span></p><p><span>It&#39;s a simple metric, but also one that finds many applications in our model.</span></p><h3 id='multibox'><span>Multibox</span></h3><p><span>Multibox is a technique for detecting objects where a prediction consists of two components –</span></p><ul><li><strong><span>Coordinates of a box that may or may not contain an object</span></strong><span>. This is a </span><em><span>regression</span></em><span> task.</span></li><li><strong><span>Scores for various object types for this box</span></strong><span>, including a </span><em><span>background</span></em><span> class which implies there is no object in the box. This is a </span><em><span>classification</span></em><span> task.</span></li></ul><h3 id='single-shot-detector-ssd'><span>Single Shot Detector (SSD)</span></h3><p><span>The SSD is a purely convolutional neural network (CNN) that we can organize into three parts –</span></p><ul><li><strong><span>Base convolutions</span></strong><span> derived from an existing image classification architecture that will provide lower-level feature maps.</span></li><li><strong><span>Auxiliary convolutions</span></strong><span> added on top of the base network that will provide higher-level feature maps.</span></li><li><strong><span>Prediction convolutions</span></strong><span> that will locate and identify objects in these feature maps.</span></li></ul><p><span>The paper demonstrates two variants of the model called the SSD300 and the SSD512. The suffixes represent the size of the input image. Although the two networks differ slightly in the way they are constructed, they are in principle the same. The SSD512 is just a larger network and results in marginally better performance.</span></p><p><span>For convenience, we will deal with the SSD300.</span></p><h3 id='base-convolutions----part-1'><span>Base Convolutions – part 1</span></h3><p><span>First of all, why use convolutions from an existing network architecture?</span></p><p><span>Because models proven to work well with image classification are already pretty good at capturing the basic essence of an image. The same convolutional features are useful for object detection, albeit in a more </span><em><span>local</span></em><span> sense – we&#39;re less interested in the image as a whole than specific regions of it where objects are present.</span></p><p><span>There&#39;s also the added advantage of being able to use layers pretrained on a reliable classification dataset. As you may know, this is called </span><strong><span>Transfer Learning</span></strong><span>. By borrowing knowledge from a different but closely related task, we&#39;ve made progress before we&#39;ve even begun.</span></p><p><span>The authors of the paper employ the </span><strong><span>VGG-16 architecture</span></strong><span> as their base network. It&#39;s rather simple in its original form.</span></p><p><img src="./img/vgg16.PNG" referrerpolicy="no-referrer"></p><p><span>They recommend using one that&#39;s pretrained on the </span><em><span>ImageNet Large Scale Visual Recognition Competition (ILSVRC)</span></em><span> classification task. Luckily, there&#39;s one already available in PyTorch, as are other popular architectures. If you wish, you could opt for something larger like the ResNet. Just be mindful of the computational requirements.  </span></p><p><span>As per the paper, </span><strong><span>we&#39;ve to make some changes to this pretrained network</span></strong><span> to adapt it to our own challenge of object detection. Some are logical and necessary, while others are mostly a matter of convenience or preference.</span></p><ul><li><span>The </span><strong><span>input image size</span></strong><span> will be </span><code>300, 300</code><span>, as stated earlier.</span></li><li><span>The </span><strong><span>3rd pooling layer</span></strong><span>, which halves dimensions, will use the mathematical </span><code>ceiling</code><span> function instead of the default </span><code>floor</code><span> function in determining output size. This is significant only if the dimensions of the preceding feature map are odd and not even. By looking at the image above, you could calculate that for our input image size of </span><code>300, 300</code><span>, the </span><code>conv3_3</code><span> feature map will be of cross-section </span><code>75, 75</code><span>, which is halved to </span><code>38, 38</code><span> instead of an inconvenient </span><code>37, 37</code><span>.</span></li><li><span>We modify the </span><strong><span>5th pooling layer</span></strong><span> from a </span><code>2, 2</code><span> kernel and </span><code>2</code><span> stride to a </span><code>3, 3</code><span> kernel and </span><code>1</code><span> stride. The effect this has is it no longer halves the dimensions of the feature map from the preceding convolutional layer.</span></li><li><span>We don&#39;t need the fully connected (i.e. classification) layers because they serve no purpose here. We will toss </span><code>fc8</code><span> away completely, but choose to </span><strong><em><span>rework</span></em><span> </span><code>fc6</code><span> and </span><code>fc7</code><span> into convolutional layers </span><code>conv6</code><span> and </span><code>conv7</code></strong><span>.</span></li></ul><p><span>The first three modifications are straightforward enough, but that last one probably needs some explaining.</span></p><h3 id='fc-→-convolutional-layer'><span>FC → Convolutional Layer</span></h3><p><span>How do we reparameterize a fully connected layer into a convolutional layer?</span></p><p><span>Consider the following scenario.</span></p><p><span>In the typical image classification setting, the first fully connected layer cannot operate on the preceding feature map or image </span><em><span>directly</span></em><span>. We&#39;d need to flatten it into a 1D structure.</span></p><p><img src="./img/fcconv1.jpg" referrerpolicy="no-referrer"></p><p><span>In this example, there&#39;s an image of dimensions </span><code>2, 2, 3</code><span>, flattened to a 1D vector of size </span><code>12</code><span>. For an output of size </span><code>2</code><span>, the fully connected layer computes two dot-products of this flattened image with two vectors of the same size </span><code>12</code><span>. </span><strong><span>These two vectors, shown in gray, are the parameters of the fully connected layer.</span></strong></p><p><span>Now, consider a different scenario where we use a convolutional layer to produce </span><code>2</code><span> output values.</span></p><p><img src="./img/fcconv2.jpg" referrerpolicy="no-referrer"></p><p><span>Here, the image of dimensions </span><code>2, 2, 3</code><span> need not be flattened, obviously. The convolutional layer uses two filters with </span><code>12</code><span> elements in the same shape as the image to perform two dot products. </span><strong><span>These two filters, shown in gray, are the parameters of the convolutional layer.</span></strong></p><p><span>But here&#39;s the key part – </span><strong><span>in both scenarios, the outputs </span><code>Y_0</code><span> and </span><code>Y_1</code><span> are the same!</span></strong></p><p><img src="./img/fcconv3.jpg" referrerpolicy="no-referrer"></p><p><span>The two scenarios are equivalent.</span></p><p><span>What does this tell us?</span></p><p><span>That </span><strong><span>on an image of size </span><code>H, W</code><span> with </span><code>I</code><span> input channels, a fully connected layer of output size </span><code>N</code><span> is equivalent to a convolutional layer with kernel size equal to the image size </span><code>H, W</code><span> and </span><code>N</code><span> output channels</span></strong><span>, provided that the parameters of the fully connected network </span><code>N, H * W * I</code><span> are the same as the parameters of the convolutional layer </span><code>N, H, W, I</code><span>.</span></p><p><img src="./img/fcconv4.jpg" referrerpolicy="no-referrer"></p><p><span>Therefore, any fully connected layer can be converted to an equivalent convolutional layer simply </span><strong><span>by reshaping its parameters</span></strong><span>.</span></p><h3 id='base-convolutions----part-2'><span>Base Convolutions – part 2</span></h3><p><span>We now know how to convert </span><code>fc6</code><span> and </span><code>fc7</code><span> in the original VGG-16 architecture into </span><code>conv6</code><span> and </span><code>conv7</code><span> respectively.</span></p><p><span>In the ImageNet VGG-16 </span><a href='https://github.com/sgrvinod/a-PyTorch-Tutorial-to-Object-Detection#base-convolutions--part-1'><span>shown previously</span></a><span>, which operates on images of size </span><code>224, 224, 3</code><span>, you can see that the output of </span><code>conv5_3</code><span> will be of size </span><code>7, 7, 512</code><span>. Therefore –</span></p><ul><li><code>fc6</code><span> with a flattened input size of </span><code>7 * 7 * 512</code><span> and an output size of </span><code>4096</code><span> has parameters of dimensions </span><code>4096, 7 * 7 * 512</code><span>. </span><strong><span>The equivalent convolutional layer </span><code>conv6</code><span> has a </span><code>7, 7</code><span> kernel size and </span><code>4096</code><span> output channels, with reshaped parameters of dimensions </span><code>4096, 7, 7, 512</code><span>.</span></strong></li><li><code>fc7</code><span> with an input size of </span><code>4096</code><span> (i.e. the output size of </span><code>fc6</code><span>) and an output size </span><code>4096</code><span> has parameters of dimensions </span><code>4096, 4096</code><span>. The input could be considered as a </span><code>1, 1</code><span> image with </span><code>4096</code><span> input channels. </span><strong><span>The equivalent convolutional layer </span><code>conv7</code><span> has a </span><code>1, 1</code><span> kernel size and </span><code>4096</code><span> output channels, with reshaped parameters of dimensions </span><code>4096, 1, 1, 4096</code><span>.</span></strong></li></ul><p><span>We can see that </span><code>conv6</code><span> has </span><code>4096</code><span> filters, each with dimensions </span><code>7, 7, 512</code><span>, and </span><code>conv7</code><span> has </span><code>4096</code><span> filters, each with dimensions </span><code>1, 1, 4096</code><span>.</span></p><p><span>These filters are numerous and large – and computationally expensive.</span></p><p><span>To remedy this, the authors opt to </span><strong><span>reduce both their number and the size of each filter by subsampling parameters</span></strong><span> from the converted convolutional layers.</span></p><ul><li><code>conv6</code><span> will use </span><code>1024</code><span> filters, each with dimensions </span><code>3, 3, 512</code><span>. Therefore, the parameters are subsampled from </span><code>4096, 7, 7, 512</code><span> to </span><code>1024, 3, 3, 512</code><span>.</span></li><li><code>conv7</code><span> will use </span><code>1024</code><span> filters, each with dimensions </span><code>1, 1, 1024</code><span>. Therefore, the parameters are subsampled from </span><code>4096, 1, 1, 4096</code><span> to </span><code>1024, 1, 1, 1024</code><span>.</span></li></ul><p><span>Based on the references in the paper, we will </span><strong><span>subsample by picking every </span><code>m</code><span>th parameter along a particular dimension</span></strong><span>, in a process known as </span><a href='https://en.wikipedia.org/wiki/Downsampling_(signal_processing)'><em><span>decimation</span></em></a><span>.  </span></p><p><span>Since the kernel of </span><code>conv6</code><span> is decimated from </span><code>7, 7</code><span> to </span><code>3,  3</code><span> by keeping only every 3rd value, there are now </span><em><span>holes</span></em><span> in the kernel. Therefore, we would need to </span><strong><span>make the kernel dilated or </span><em><span>atrous</span></em></strong><span>.</span></p><p><span>This corresponds to a dilation of </span><code>3</code><span> (same as the decimation factor </span><code>m = 3</code><span>). However, the authors actually use a dilation of </span><code>6</code><span>, possibly because the 5th pooling layer no longer halves the dimensions of the preceding feature map.</span></p><p><span>We are now in a position to present our base network, </span><strong><span>the modified VGG-16</span></strong><span>.</span></p><p><img src="./img/modifiedvgg.PNG" referrerpolicy="no-referrer"></p><p><span>In the above figure, pay special attention to the outputs of </span><code>conv4_3</code><span> and </span><code>conv_7</code><span>. You will see why soon enough.</span></p><h3 id='auxiliary-convolutions-1'><span>Auxiliary Convolutions</span></h3><p><span>We will now </span><strong><span>stack some more convolutional layers on top of our base network</span></strong><span>. These convolutions provide additional feature maps, each progressively smaller than the last.</span></p><p><img src="./img/auxconv.jpg" referrerpolicy="no-referrer"></p><p><span>We introduce four convolutional blocks, each with two layers. While size reduction happened through pooling in the base network, here it is facilitated by a stride of </span><code>2</code><span> in every second layer.</span></p><p><span>Again, take note of the feature maps from </span><code>conv8_2</code><span>, </span><code>conv9_2</code><span>, </span><code>conv10_2</code><span>, and </span><code>conv11_2</code><span>.</span></p><h3 id='a-detour'><span>A detour</span></h3><p><span>Before we move on to the prediction convolutions, we must first understand what it is we are predicting. Sure, it&#39;s objects and their positions, </span><em><span>but in what form?</span></em></p><p><span>It is here that we must learn about </span><em><span>priors</span></em><span> and the crucial role they play in the SSD.</span></p><h4 id='priors-1'><span>Priors</span></h4><p><span>Object predictions can be quite diverse, and I don&#39;t just mean their type. They can occur at any position, with any size and shape. Mind you, we shouldn&#39;t go as far as to say there are </span><em><span>infinite</span></em><span> possibilities for where and how an object can occur. While this may be true mathematically, many options are simply improbable or uninteresting. Furthermore, we needn&#39;t insist that boxes are pixel-perfect.</span></p><p><span>In effect, we can discretize the mathematical space of potential predictions into just </span><em><span>thousands</span></em><span> of possibilities.</span></p><p><strong><span>Priors are precalculated, fixed boxes which collectively represent this universe of probable and approximate box predictions</span></strong><span>.</span></p><p><span>Priors are manually but carefully chosen based on the shapes and sizes of ground truth objects in our dataset. By placing these priors at every possible location in a feature map, we also account for variety in position.</span></p><p><span>In defining the priors, the authors specify that –</span></p><ul><li><strong><span>they will be applied to various low-level and high-level feature maps</span></strong><span>, viz. those from </span><code>conv4_3</code><span>, </span><code>conv7</code><span>, </span><code>conv8_2</code><span>, </span><code>conv9_2</code><span>, </span><code>conv10_2</code><span>, and </span><code>conv11_2</code><span>. These are the same feature maps indicated on the figures before.</span></li><li><strong><span>if a prior has a scale </span><code>s</code><span>, then its area is equal to that of a square with side </span><code>s</code></strong><span>. The largest feature map, </span><code>conv4_3</code><span>, will have priors with a scale of </span><code>0.1</code><span>, i.e. </span><code>10%</code><span> of image&#39;s dimensions, while the rest have priors with scales linearly increasing from </span><code>0.2</code><span> to </span><code>0.9</code><span>. As you can see, larger feature maps have priors with smaller scales and are therefore ideal for detecting smaller objects.</span></li><li><strong><span>At </span><em><span>each</span></em><span> position on a feature map, there will be priors of various aspect ratios</span></strong><span>. All feature maps will have priors with ratios </span><code>1:1, 2:1, 1:2</code><span>. The intermediate feature maps of </span><code>conv7</code><span>, </span><code>conv8_2</code><span>, and </span><code>conv9_2</code><span> will </span><em><span>also</span></em><span> have priors with ratios </span><code>3:1, 1:3</code><span>. Moreover, all feature maps will have </span><em><span>one extra prior</span></em><span> with an aspect ratio of </span><code>1:1</code><span> and at a scale that is the geometric mean of the scales of the current and subsequent feature map.</span></li></ul><figure><table><thead><tr><th style='text-align:center;' ><span>Feature Map From</span></th><th style='text-align:center;' ><span>Feature Map Dimensions</span></th><th style='text-align:center;' ><span>Prior Scale</span></th><th style='text-align:center;' ><span>Aspect Ratios</span></th><th style='text-align:center;' ><span>Number of Priors per Position</span></th><th style='text-align:center;' ><span>Total Number of Priors on this Feature Map</span></th></tr></thead><tbody><tr><td style='text-align:center;' ><code>conv4_3</code></td><td style='text-align:center;' ><span>38, 38</span></td><td style='text-align:center;' ><span>0.1</span></td><td style='text-align:center;' ><span>1:1, 2:1, 1:2 + an extra prior</span></td><td style='text-align:center;' ><span>4</span></td><td style='text-align:center;' ><span>5776</span></td></tr><tr><td style='text-align:center;' ><code>conv7</code></td><td style='text-align:center;' ><span>19, 19</span></td><td style='text-align:center;' ><span>0.2</span></td><td style='text-align:center;' ><span>1:1, 2:1, 1:2, 3:1, 1:3 + an extra prior</span></td><td style='text-align:center;' ><span>6</span></td><td style='text-align:center;' ><span>2166</span></td></tr><tr><td style='text-align:center;' ><code>conv8_2</code></td><td style='text-align:center;' ><span>10, 10</span></td><td style='text-align:center;' ><span>0.375</span></td><td style='text-align:center;' ><span>1:1, 2:1, 1:2, 3:1, 1:3 + an extra prior</span></td><td style='text-align:center;' ><span>6</span></td><td style='text-align:center;' ><span>600</span></td></tr><tr><td style='text-align:center;' ><code>conv9_2</code></td><td style='text-align:center;' ><span>5, 5</span></td><td style='text-align:center;' ><span>0.55</span></td><td style='text-align:center;' ><span>1:1, 2:1, 1:2, 3:1, 1:3 + an extra prior</span></td><td style='text-align:center;' ><span>6</span></td><td style='text-align:center;' ><span>150</span></td></tr><tr><td style='text-align:center;' ><code>conv10_2</code></td><td style='text-align:center;' ><span>3,  3</span></td><td style='text-align:center;' ><span>0.725</span></td><td style='text-align:center;' ><span>1:1, 2:1, 1:2 + an extra prior</span></td><td style='text-align:center;' ><span>4</span></td><td style='text-align:center;' ><span>36</span></td></tr><tr><td style='text-align:center;' ><code>conv11_2</code></td><td style='text-align:center;' ><span>1, 1</span></td><td style='text-align:center;' ><span>0.9</span></td><td style='text-align:center;' ><span>1:1, 2:1, 1:2 + an extra prior</span></td><td style='text-align:center;' ><span>4</span></td><td style='text-align:center;' ><span>4</span></td></tr><tr><td style='text-align:center;' ><strong><span>Grand Total</span></strong></td><td style='text-align:center;' ><span>–</span></td><td style='text-align:center;' ><span>–</span></td><td style='text-align:center;' ><span>–</span></td><td style='text-align:center;' ><span>–</span></td><td style='text-align:center;' ><strong><span>8732 priors</span></strong></td></tr></tbody></table></figure><p><span>There are a total of 8732 priors defined for the SSD300!</span></p><h4 id='visualizing-priors'><span>Visualizing Priors</span></h4><p><span>We defined the priors in terms of their </span><em><span>scales</span></em><span> and </span><em><span>aspect ratios</span></em><span>.</span></p><p><img src="./img/wh1.jpg" referrerpolicy="no-referrer"></p><p><span>Solving these equations yields a prior&#39;s dimensions </span><code>w</code><span> and </span><code>h</code><span>.</span></p><p><img src="./img/wh2.jpg" referrerpolicy="no-referrer"></p><p><span>We&#39;re now in a position to draw them on their respective feature maps.</span></p><p><span>For example, let&#39;s try to visualize what the priors will look like at the central tile of the feature map from </span><code>conv9_2</code><span>.</span></p><p><img src="./img/priors1.jpg" referrerpolicy="no-referrer"></p><p><span>The same priors also exist for each of the other tiles.</span></p><p><img src="./img/priors2.jpg" referrerpolicy="no-referrer"></p><h4 id='predictions-vis-à-vis-priors'><span>Predictions vis-à-vis Priors</span></h4><p><a href='https://github.com/sgrvinod/a-PyTorch-Tutorial-to-Object-Detection#multibox'><span>Earlier</span></a><span>, we said we would use regression to find the coordinates of an object&#39;s bounding box. But then, surely, the priors can&#39;t represent our final predicted boxes?</span></p><p><span>They don&#39;t.</span></p><p><span>Again, I would like to reiterate that the priors represent, </span><em><span>approximately</span></em><span>, the possibilities for prediction.</span></p><p><span>This means that </span><strong><span>we use each prior as an approximate starting point and then find out how much it needs to be adjusted to obtain a more exact prediction for a bounding box</span></strong><span>.</span></p><p><span>So if each predicted bounding box is a slight deviation from a prior, and our goal is to calculate this deviation, we need a way to measure or quantify it.</span></p><p><span>Consider a cat, its predicted bounding box, and the prior with which the prediction was made.  </span></p><p><img src="./img/ecs1.PNG" referrerpolicy="no-referrer"></p><p><span>Assume they are represented in center-size coordinates, which we are familiar with.</span></p><p><span>Then –</span></p><p><img src="./img/ecs2.PNG" referrerpolicy="no-referrer"></p><p><span>This answers the question we posed at the </span><a href='https://github.com/sgrvinod/a-PyTorch-Tutorial-to-Object-Detection#a-detour'><span>beginning of this section</span></a><span>. Considering that each prior is adjusted to obtain a more precise prediction, </span><strong><span>these four offsets </span><code>(g_c_x, g_c_y, g_w, g_h)</code><span> are the form in which we will regress bounding boxes&#39; coordinates</span></strong><span>.</span></p><p><span>As you can see, each offset is normalized by the corresponding dimension of the prior. This makes sense because a certain offset would be less significant for a larger prior than it would be for a smaller prior.</span></p><h3 id='prediction-convolutions-1'><span>Prediction convolutions</span></h3><p><span>Earlier, we earmarked and defined priors for six feature maps of various scales and granularity, viz. those from </span><code>conv4_3</code><span>, </span><code>conv7</code><span>, </span><code>conv8_2</code><span>, </span><code>conv9_2</code><span>, </span><code>conv10_2</code><span>, and </span><code>conv11_2</code><span>.</span></p><p><span>Then, </span><strong><span>for </span><em><span>each</span></em><span> prior at </span><em><span>each</span></em><span> location on </span><em><span>each</span></em><span> feature map</span></strong><span>, we want to predict –</span></p><ul><li><span>the </span><strong><span>offsets </span><code>(g_c_x, g_c_y, g_w, g_h)</code></strong><span> for a bounding box.</span></li><li><span>a set of </span><strong><code>n_classes</code><span> scores</span></strong><span> for the bounding box, where </span><code>n_classes</code><span> represents the total number of object types (including a </span><em><span>background</span></em><span> class).</span></li></ul><p><span>To do this in the simplest manner possible, </span><strong><span>we need two convolutional layers for each feature map</span></strong><span> –</span></p><ul><li><p><span>a </span><strong><em><span>localization</span></em><span> prediction</span></strong><span> convolutional layer with a </span><code>3,  3</code><span> kernel evaluating at each location (i.e. with padding and stride of </span><code>1</code><span>) with </span><code>4</code><span> filters for </span><em><span>each</span></em><span> prior present at the location.</span></p><p><span>The </span><code>4</code><span> filters for a prior calculate the four encoded offsets </span><code>(g_c_x, g_c_y, g_w, g_h)</code><span> for the bounding box predicted from that prior.</span></p></li><li><p><span>a </span><strong><em><span>class</span></em><span> prediction</span></strong><span> convolutional layer with a </span><code>3,  3</code><span> kernel evaluating at each location (i.e. with padding and stride of </span><code>1</code><span>) with </span><code>n_classes</code><span> filters for </span><em><span>each</span></em><span> prior present at the location.</span></p><p><span>The </span><code>n_classes</code><span> filters for a prior calculate a set of </span><code>n_classes</code><span> scores for that prior.</span></p></li></ul><p><img src="./img/predconv1.jpg" referrerpolicy="no-referrer"></p><p><span>All our filters are applied with a kernel size of </span><code>3, 3</code><span>.</span></p><p><span>We don&#39;t really need kernels (or filters) in the same shapes as the priors because the different filters will </span><em><span>learn</span></em><span> to make predictions with respect to the different prior shapes.</span></p><p><span>Let&#39;s take a look at the </span><strong><span>outputs of these convolutions</span></strong><span>. Consider again the feature map from </span><code>conv9_2</code><span>.</span></p><p><img src="./img/predconv2.jpg" referrerpolicy="no-referrer"></p><p><span>The outputs of the localization and class prediction layers are shown in blue and yellow respectively. You can see that the cross-section (</span><code>5, 5</code><span>) remains unchanged.</span></p><p><span>What we&#39;re really interested in is the </span><em><span>third</span></em><span> dimension, i.e. the channels. These contain the actual predictions.</span></p><p><span>If you </span><strong><span>choose a tile, </span><em><span>any</span></em><span> tile, in the localization predictions and expand it</span></strong><span>, what will you see?</span></p><p><img src="./img/predconv3.jpg" referrerpolicy="no-referrer"></p><p><span>Voilà! The channel values at each position of the localization predictions represent the encoded offsets with respect to the priors at that position.</span></p><p><span>Now, </span><strong><span>do the same with the class predictions.</span></strong><span> Assume </span><code>n_classes = 3</code><span>.</span></p><p><img src="./img/predconv4.jpg" referrerpolicy="no-referrer"></p><p><span>Similar to before, these channels represent the class scores for the priors at that position.</span></p><p><span>Now that we understand what the predictions for the feature map from </span><code>conv9_2</code><span> look like, we can </span><strong><span>reshape them into a more amenable form.</span></strong></p><p><img src="./img/reshaping1.jpg" referrerpolicy="no-referrer"></p><p><span>We have arranged the </span><code>150</code><span> predictions serially. To the human mind, this should appear more intuitive.</span></p><p><span>But let&#39;s not stop here. We could do the same for the predictions for </span><em><span>all</span></em><span> layers and stack them together.</span></p><p><span>We calculated earlier that there are a total of 8732 priors defined for our model. Therefore, there will be </span><strong><span>8732 predicted boxes in encoded-offset form, and 8732 sets of class scores</span></strong><span>.</span></p><p><img src="./img/reshaping2.jpg" referrerpolicy="no-referrer"></p><p><strong><span>This is the final output of the prediction stage.</span></strong><span> A stack of boxes, if you will, and estimates for what&#39;s in them.</span></p><p><span>It&#39;s all coming together, isn&#39;t it? If this is your first rodeo in object detection, I should think there&#39;s now a faint light at the end of the tunnel.</span></p><h3 id='multibox-loss-1'><span>Multibox loss</span></h3><p><span>Based on the nature of our predictions, it&#39;s easy to see why we might need a unique loss function. Many of us have calculated losses in regression or classification settings before, but rarely, if ever, </span><em><span>together</span></em><span>.</span></p><p><span>Obviously, our total loss must be an </span><strong><span>aggregate of losses from both types of predictions</span></strong><span> – bounding box localizations and class scores.</span></p><p><span>Then, there are a few questions to be answered –</span></p><blockquote><p><em><span>What loss function will be used for the regressed bounding boxes?</span></em></p></blockquote><blockquote><p><em><span>Will we use multiclass cross-entropy for the class scores?</span></em></p></blockquote><blockquote><p><em><span>In what ratio will we combine them?</span></em></p></blockquote><blockquote><p><em><span>How do we match predicted boxes to their ground truths?</span></em></p></blockquote><blockquote><p><em><span>We have 8732 predictions! Won&#39;t most of these contain no object? Do we even consider them?</span></em></p></blockquote><p><span>Phew. Let&#39;s get to work.</span></p><h4 id='matching-predictions-to-ground-truths'><span>Matching predictions to ground truths</span></h4><p><span>Remember, the nub of any supervised learning algorithm is that </span><strong><span>we need to be able to match predictions to their ground truths</span></strong><span>. This is tricky since object detection is more open-ended than the average learning task.</span></p><p><span>For the model to learn </span><em><span>anything</span></em><span>, we&#39;d need to structure the problem in a way that allows for comparisions between our predictions and the objects actually present in the image.</span></p><p><span>Priors enable us to do exactly this!</span></p><ul><li><strong><span>Find the Jaccard overlaps</span></strong><span> between the 8732 priors and </span><code>N</code><span> ground truth objects. This will be a tensor of size </span><code>8732, N</code><span>.</span></li><li><strong><span>Match</span></strong><span> each of the 8732 priors to the object with which it has the greatest overlap.</span></li><li><span>If a prior is matched with an object with a </span><strong><span>Jaccard overlap of less than </span><code>0.5</code></strong><span>, then it cannot be said to &quot;contain&quot; the object, and is therefore a </span><strong><em><span>negative</span></em><span> match</span></strong><span>. Considering we have thousands of priors, most priors will test negative for an object.</span></li><li><span>On the other hand, a handful of priors will actually </span><strong><span>overlap significantly (greater than </span><code>0.5</code><span>)</span></strong><span> with an object, and can be said to &quot;contain&quot; that object. These are </span><strong><em><span>positive</span></em><span> matches</span></strong><span>.</span></li><li><span>Now that we have </span><strong><span>matched each of the 8732 priors to a ground truth</span></strong><span>, we have, in effect, also </span><strong><span>matched the corresponding 8732 predictions to a ground truth</span></strong><span>.  </span></li></ul><p><span>Let&#39;s reproduce this logic with an example.</span></p><p><img src="./img/matching1.PNG" referrerpolicy="no-referrer"></p><p><span>For convenience, we will assume there are just seven priors, shown in red. The ground truths are in yellow – there are three actual objects in this image.</span></p><p><span>Following the steps outlined earlier will yield the following matches –</span></p><p><img src="./img/matching2.jpg" referrerpolicy="no-referrer"></p><p><span>Now, </span><strong><span>each prior has a match</span></strong><span>, positive or negative. By extension, </span><strong><span>each prediction has a match</span></strong><span>, positive or negative.</span></p><p><span>Predictions that are positively matched with an object now have ground truth coordinates that will serve as </span><strong><span>targets for localization</span></strong><span>, i.e. in the </span><em><span>regression</span></em><span> task. Naturally, there will be no target coordinates for negative matches.</span></p><p><span>All predictions have a ground truth label, which is either the type of object if it is a positive match or a </span><em><span>background</span></em><span> class if it is a negative match. These are used as </span><strong><span>targets for class prediction</span></strong><span>, i.e. the </span><em><span>classification</span></em><span> task.</span></p><h4 id='localization-loss'><span>Localization loss</span></h4><p><span>We have </span><strong><span>no ground truth coordinates for the negative matches</span></strong><span>. This makes perfect sense. Why train the model to draw boxes around empty space?</span></p><p><span>Therefore, the localization loss is computed only on how accurately we regress positively matched predicted boxes to the corresponding ground truth coordinates.</span></p><p><span>Since we predicted localization boxes in the form of offsets </span><code>(g_c_x, g_c_y, g_w, g_h)</code><span>, we would also need to encode the ground truth coordinates accordingly before we calculate the loss.</span></p><p><span>The localization loss is the averaged </span><strong><span>Smooth L1</span></strong><span> loss between the encoded offsets of positively matched localization boxes and their ground truths.</span></p><p><img src="./img/locloss.jpg" referrerpolicy="no-referrer"></p><h4 id='confidence-loss'><span>Confidence loss</span></h4><p><span>Every prediction, no matter positive or negative, has a ground truth label associated with it. It is important that the model recognizes both objects and a lack of them.</span></p><p><span>However, considering that there are usually only a handful of objects in an image, </span><strong><span>the vast majority of the thousands of predictions we made do not actually contain an object</span></strong><span>. As Walter White would say, </span><em><span>tread lightly</span></em><span>. If the negative matches overwhelm the positive ones, we will end up with a model that is less likely to detect objects because, more often than not, it is taught to detect the </span><em><span>background</span></em><span> class.</span></p><p><span>The solution may be obvious – limit the number of negative matches that will be evaluated in the loss function. But how do we choose?</span></p><p><span>Well, why not use the ones that the model was most </span><em><span>wrong</span></em><span> about? In other words, only use those predictions where the model found it hardest to recognize that there are no objects. This is called </span><strong><span>Hard Negative Mining</span></strong><span>.</span></p><p><span>The number of hard negatives we will use, say </span><code>N_hn</code><span>, is usually a fixed multiple of the number of positive matches for this image. In this particular case, the authors have decided to use three times as many hard negatives, i.e. </span><code>N_hn = 3 * N_p</code><span>. The hardest negatives are discovered by finding the Cross Entropy loss for each negatively matched prediction and choosing those with top </span><code>N_hn</code><span> losses.</span></p><p><span>Then, the confidence loss is simply the sum of the </span><strong><span>Cross Entropy</span></strong><span> losses among the positive and hard negative matches.</span></p><p><img src="./img/confloss.jpg" referrerpolicy="no-referrer"></p><p><span>You will notice that it is averaged by the number of positive matches.</span></p><h4 id='total-loss'><span>Total loss</span></h4><p><span>The </span><strong><span>Multibox loss is the aggregate of the two losses</span></strong><span>, combined in a ratio </span><code>α</code><span>.</span></p><p><img src="./img/totalloss.jpg" referrerpolicy="no-referrer"></p><p><span>In general, we needn&#39;t decide on a value for </span><code>α</code><span>. It could be a learnable parameter.</span></p><p><span>For the SSD, however, the authors simply use </span><code>α = 1</code><span>, i.e. add the two losses. We&#39;ll take it!</span></p><h3 id='processing-predictions'><span>Processing predictions</span></h3><p><span>After the model is trained, we can apply it to images. However, the predictions are still in their raw form – two tensors containing the offsets and class scores for 8732 priors. These would need to be processed to </span><strong><span>obtain final, human-interpretable bounding boxes with labels.</span></strong></p><p><span>This entails the following –</span></p><ul><li><p><span>We have 8732 predicted boxes represented as offsets </span><code>(g_c_x, g_c_y, g_w, g_h)</code><span> from their respective priors. Decode them to boundary coordinates, which are actually directly interpretable.</span></p></li><li><p><span>Then, for each </span><em><span>non-background</span></em><span> class,</span></p><ul><li><span>Extract the scores for this class for each of the 8732 boxes.</span></li><li><span>Eliminate boxes that do not meet a certain threshold for this score.</span></li><li><span>The remaining (uneliminated) boxes are candidates for this particular class of object.</span></li></ul></li></ul><p><span>At this point, if you were to draw these candidate boxes on the original image, you&#39;d see </span><strong><span>many highly overlapping boxes that are obviously redundant</span></strong><span>. This is because it&#39;s extremely likely that, from the thousands of priors at our disposal, more than one prediction corresponds to the same object.</span></p><p><span>For instance, consider the image below.</span></p><p><img src="./img/nms1.PNG" referrerpolicy="no-referrer"></p><p><span>There&#39;s clearly only three objects in it – two dogs and a cat. But according to the model, there are </span><em><span>three</span></em><span> dogs and </span><em><span>two</span></em><span> cats.</span></p><p><span>Mind you, this is just a mild example. It could really be much, much worse.</span></p><p><span>Now, to you, it may be obvious which boxes are referring to the same object. This is because your mind can process that certain boxes coincide significantly with each other and a specific object.</span></p><p><span>In practice, how would this be done?</span></p><p><span>First, </span><strong><span>line up the candidates for each class in terms of how </span><em><span>likely</span></em><span> they are</span></strong><span>.</span></p><p><img src="./img/nms2.PNG" referrerpolicy="no-referrer"></p><p><span>We&#39;ve sorted them by their scores.</span></p><p><span>The next step is to find which candidates are redundant. We already have a tool at our disposal to judge how much two boxes have in common with each other – the Jaccard overlap.</span></p><p><span>So, if we were to </span><strong><span>draw up the Jaccard similarities between all the candidates in a given class</span></strong><span>, we could evaluate each pair and </span><strong><span>if found to overlap significantly, keep only the </span><em><span>more likely</span></em><span> candidate</span></strong><span>.</span></p><p><img src="./img/nms3.jpg" referrerpolicy="no-referrer"></p><p><span>Thus, we&#39;ve eliminated the rogue candidates – one of each animal.</span></p><p><span>This process is called </span><strong><span>Non-Maximum Suppression (NMS)</span></strong><span> because when multiple candidates are found to overlap significantly with each other such that they could be referencing the same object, </span><strong><span>we suppress all but the one with the maximum score</span></strong><span>.</span></p><p><span>Algorithmically, it is carried out as follows –</span></p><ul><li><p><span>Upon selecting candidates for each </span><em><span>non-background</span></em><span> class,</span></p><ul><li><span>Arrange candidates for this class in order of decreasing likelihood.</span></li><li><span>Consider the candidate with the highest score. Eliminate all candidates with lesser scores that have a Jaccard overlap of more than, say, </span><code>0.5</code><span> with this candidate.</span></li><li><span>Consider the next highest-scoring candidate still remaining in the pool. Eliminate all candidates with lesser scores that have a Jaccard overlap of more than </span><code>0.5</code><span> with this candidate.</span></li><li><span>Repeat until you run through the entire sequence of candidates.</span></li></ul></li></ul><p><span>The end result is that you will have just a single box – the very best one – for each object in the image.</span></p><p><img src="./img/nms4.PNG" referrerpolicy="no-referrer"></p><p><span>Non-Maximum Suppression is quite crucial for obtaining quality detections.</span></p><p><span>Happily, it&#39;s also the final step.</span></p><h1 id='implementation'><span>Implementation</span></h1><p><span>The sections below briefly describe the implementation.</span></p><p><span>They are meant to provide some context, but </span><strong><span>details are best understood directly from the code</span></strong><span>, which is quite heavily commented.</span></p><h3 id='dataset'><span>Dataset</span></h3><p><span>We will use Pascal Visual Object Classes (VOC) data from the years 2007 and 2012.</span></p><h4 id='description'><span>Description</span></h4><p><span>This data contains images with twenty different types of objects.</span></p><pre class="md-fences md-end-block ty-contain-cm modeLoaded" spellcheck="false" lang="python"><div class="CodeMirror cm-s-inner cm-s-null-scroll CodeMirror-wrap" lang="python"><div style="overflow: hidden; position: relative; width: 3px; height: 0px; top: 9.25px; left: 8px;"><textarea autocorrect="off" autocapitalize="off" spellcheck="false" tabindex="0" style="position: absolute; bottom: -1em; padding: 0px; width: 1000px; height: 1em; outline: none;"></textarea></div><div class="CodeMirror-scrollbar-filler" cm-not-content="true"></div><div class="CodeMirror-gutter-filler" cm-not-content="true"></div><div class="CodeMirror-scroll" tabindex="-1"><div class="CodeMirror-sizer" style="margin-left: 0px; margin-bottom: 0px; border-right-width: 0px; padding-right: 0px; padding-bottom: 0px;"><div style="position: relative; top: 0px;"><div class="CodeMirror-lines" role="presentation"><div role="presentation" style="position: relative; outline: none;"><div class="CodeMirror-measure"><pre><span>xxxxxxxxxx</span></pre></div><div class="CodeMirror-measure"></div><div style="position: relative; z-index: 1;"></div><div class="CodeMirror-code" role="presentation"><div class="CodeMirror-activeline" style="position: relative;"><div class="CodeMirror-activeline-background CodeMirror-linebackground"></div><div class="CodeMirror-gutter-background CodeMirror-activeline-gutter" style="left: 0px; width: 0px;"></div><pre class=" CodeMirror-line " role="presentation"><span role="presentation" style="padding-right: 0.1px;">{<span class="cm-string">'aeroplane'</span>, <span class="cm-string">'bicycle'</span>, <span class="cm-string">'bird'</span>, <span class="cm-string">'boat'</span>, <span class="cm-string">'bottle'</span>, <span class="cm-string">'bus'</span>, <span class="cm-string">'car'</span>, <span class="cm-string">'cat'</span>, <span class="cm-string">'chair'</span>, <span class="cm-string">'cow'</span>, <span class="cm-string">'diningtable'</span>, <span class="cm-string">'dog'</span>, <span class="cm-string">'horse'</span>, <span class="cm-string">'motorbike'</span>, <span class="cm-string">'person'</span>, <span class="cm-string">'pottedplant'</span>, <span class="cm-string">'sheep'</span>, <span class="cm-string">'sofa'</span>, <span class="cm-string">'train'</span>, <span class="cm-string">'tvmonitor'</span>}</span></pre></div></div></div></div></div></div><div style="position: absolute; height: 0px; width: 1px; border-bottom: 0px solid transparent; top: 69px;"></div><div class="CodeMirror-gutters" style="display: none; height: 69px;"></div></div></div></pre><p><span>Each image can contain one or more ground truth objects.</span></p><p><span>Each object is represented by –</span></p><ul><li><span>a bounding box in absolute boundary coordinates</span></li><li><span>a label (one of the object types mentioned above)</span></li><li><span>a perceived detection difficulty (either </span><code>0</code><span>, meaning </span><em><span>not difficult</span></em><span>, or </span><code>1</code><span>, meaning </span><em><span>difficult</span></em><span>)</span></li></ul><h4 id='download'><span>Download</span></h4><p><span>Specfically, you will need to download the following VOC datasets –</span></p><ul><li><a href='http://host.robots.ox.ac.uk/pascal/VOC/voc2007/VOCtrainval_06-Nov-2007.tar'><span>2007 </span><em><span>trainval</span></em></a><span> (460MB)</span></li><li><a href='http://host.robots.ox.ac.uk/pascal/VOC/voc2012/VOCtrainval_11-May-2012.tar'><span>2012 </span><em><span>trainval</span></em></a><span> (2GB)</span></li><li><a href='http://host.robots.ox.ac.uk/pascal/VOC/voc2007/VOCtest_06-Nov-2007.tar'><span>2007 </span><em><span>test</span></em></a><span> (451MB)</span></li></ul><p><span>Consistent with the paper, the two </span><em><span>trainval</span></em><span> datasets are to be used for training, while the VOC 2007 </span><em><span>test</span></em><span> will serve as our test data.  </span></p><p><span>Make sure you extract both the VOC 2007 </span><em><span>trainval</span></em><span> and 2007 </span><em><span>test</span></em><span> data to the same location, i.e. merge them.</span></p><h3 id='inputs-to-model'><span>Inputs to model</span></h3><p><span>We will need three inputs.</span></p><h4 id='images'><span>Images</span></h4><p><span>Since we&#39;re using the SSD300 variant, the images would need to be sized at </span><code>300, 300</code><span> pixels and in the RGB format.</span></p><p><span>Remember, we&#39;re using a VGG-16 base pretrained on ImageNet that is already available in PyTorch&#39;s </span><code>torchvision</code><span> module. </span><a href='https://pytorch.org/docs/master/torchvision/models.html'><span>This page</span></a><span> details the preprocessing or transformation we would need to perform in order to use this model – pixel values must be in the range [0,1] and we must then normalize the image by the mean and standard deviation of the ImageNet images&#39; RGB channels.</span></p><pre class="md-fences md-end-block ty-contain-cm modeLoaded" spellcheck="false" lang="python"><div class="CodeMirror cm-s-inner cm-s-null-scroll CodeMirror-wrap" lang="python"><div style="overflow: hidden; position: relative; width: 3px; height: 0px; top: 9.5px; left: 8px;"><textarea autocorrect="off" autocapitalize="off" spellcheck="false" tabindex="0" style="position: absolute; bottom: -1em; padding: 0px; width: 1000px; height: 1em; outline: none;"></textarea></div><div class="CodeMirror-scrollbar-filler" cm-not-content="true"></div><div class="CodeMirror-gutter-filler" cm-not-content="true"></div><div class="CodeMirror-scroll" tabindex="-1"><div class="CodeMirror-sizer" style="margin-left: 0px; margin-bottom: 0px; border-right-width: 0px; padding-right: 0px; padding-bottom: 0px;"><div style="position: relative; top: 0px;"><div class="CodeMirror-lines" role="presentation"><div role="presentation" style="position: relative; outline: none;"><div class="CodeMirror-measure"><pre><span>xxxxxxxxxx</span></pre></div><div class="CodeMirror-measure"></div><div style="position: relative; z-index: 1;"></div><div class="CodeMirror-code" role="presentation"><div class="CodeMirror-activeline" style="position: relative;"><div class="CodeMirror-activeline-background CodeMirror-linebackground"></div><div class="CodeMirror-gutter-background CodeMirror-activeline-gutter" style="left: 0px; width: 0px;"></div><pre class=" CodeMirror-line " role="presentation"><span role="presentation" style="padding-right: 0.1px;"><span class="cm-variable">mean</span> = [<span class="cm-number">0.485</span>, <span class="cm-number">0.456</span>, <span class="cm-number">0.406</span>]</span></pre></div><pre class=" CodeMirror-line " role="presentation"><span role="presentation" style="padding-right: 0.1px;"><span class="cm-variable">std</span> = [<span class="cm-number">0.229</span>, <span class="cm-number">0.224</span>, <span class="cm-number">0.225</span>]</span></pre></div></div></div></div></div><div style="position: absolute; height: 0px; width: 1px; border-bottom: 0px solid transparent; top: 46px;"></div><div class="CodeMirror-gutters" style="display: none; height: 46px;"></div></div></div></pre><p><span>Also, PyTorch follows the NCHW convention, which means the channels dimension (C) must precede the size dimensions.</span></p><p><span>Therefore, </span><strong><span>images fed to the model must be a </span><code>Float</code><span> tensor of dimensions </span><code>N, 3, 300, 300</code></strong><span>, and must be normalized by the aforesaid mean and standard deviation. </span><code>N</code><span> is the batch size.</span></p><h4 id='objects-bounding-boxes'><span>Objects&#39; Bounding Boxes</span></h4><p><span>We would need to supply, for each image, the bounding boxes of the ground truth objects present in it in fractional boundary coordinates </span><code>(x_min, y_min, x_max, y_max)</code><span>.</span></p><p><span>Since the number of objects in any given image can vary, we can&#39;t use a fixed size tensor for storing the bounding boxes for the entire batch of </span><code>N</code><span> images.</span></p><p><span>Therefore, </span><strong><span>ground truth bounding boxes fed to the model must be a list of length </span><code>N</code><span>, where each element of the list is a </span><code>Float</code><span> tensor of dimensions </span><code>N_o, 4</code></strong><span>, where </span><code>N_o</code><span> is the number of objects present in that particular image.</span></p><h4 id='objects-labels'><span>Objects&#39; Labels</span></h4><p><span>We would need to supply, for each image, the labels of the ground truth objects present in it.</span></p><p><span>Each label would need to be encoded as an integer from </span><code>1</code><span> to </span><code>20</code><span> representing the twenty different object types. In addition, we will add a </span><em><span>background</span></em><span> class with index </span><code>0</code><span>, which indicates the absence of an object in a bounding box. (But naturally, this label will not actually be used for any of the ground truth objects in the dataset.)</span></p><p><span>Again, since the number of objects in any given image can vary, we can&#39;t use a fixed size tensor for storing the labels for the entire batch of </span><code>N</code><span> images.</span></p><p><span>Therefore, </span><strong><span>ground truth labels fed to the model must be a list of length </span><code>N</code><span>, where each element of the list is a </span><code>Long</code><span> tensor of dimensions </span><code>N_o</code></strong><span>, where </span><code>N_o</code><span> is the number of objects present in that particular image.</span></p><h3 id='data-pipeline'><span>Data pipeline</span></h3><p><span>As you know, our data is divided into </span><em><span>training</span></em><span> and </span><em><span>test</span></em><span> splits.</span></p><h4 id='parse-raw-data'><span>Parse raw data</span></h4><p><span>See </span><code>create_data_lists()</code><span> in </span><a href='https://github.com/sgrvinod/a-PyTorch-Tutorial-to-Object-Detection/blob/master/utils.py'><code>utils.py</code></a><span>.</span></p><p><span>This parses the data downloaded and saves the following files –</span></p><ul><li><span>A </span><strong><span>JSON file for each split with a list of the absolute filepaths of </span><code>I</code><span> images</span></strong><span>, where </span><code>I</code><span> is the total number of images in the split.</span></li><li><span>A </span><strong><span>JSON file for each split with a list of </span><code>I</code><span> dictionaries containing ground truth objects, i.e. bounding boxes in absolute boundary coordinates, their encoded labels, and perceived detection difficulties</span></strong><span>. The </span><code>i</code><span>th dictionary in this list will contain the objects present in the </span><code>i</code><span>th image in the previous JSON file.</span></li><li><span>A </span><strong><span>JSON file which contains the </span><code>label_map</code></strong><span>, the label-to-index dictionary with which the labels are encoded in the previous JSON file. This dictionary is also available in </span><a href='https://github.com/sgrvinod/a-PyTorch-Tutorial-to-Object-Detection/blob/master/utils.py'><code>utils.py</code></a><span> and directly importable.</span></li></ul><h4 id='pytorch-dataset'><span>PyTorch Dataset</span></h4><p><span>See </span><code>PascalVOCDataset</code><span> in </span><a href='https://github.com/sgrvinod/a-PyTorch-Tutorial-to-Object-Detection/blob/master/datasets.py'><code>datasets.py</code></a><span>.</span></p><p><span>This is a subclass of PyTorch </span><a href='https://pytorch.org/docs/master/data.html#torch.utils.data.Dataset'><code>Dataset</code></a><span>, used to </span><strong><span>define our training and test datasets.</span></strong><span> It needs a </span><code>__len__</code><span> method defined, which returns the size of the dataset, and a </span><code>__getitem__</code><span> method which returns the </span><code>i</code><span>th image, bounding boxes of the objects in this image, and labels for the objects in this image, using the JSON files we saved earlier.</span></p><p><span>You will notice that it also returns the perceived detection difficulties of each of these objects, but these are not actually used in training the model. They are required only in the </span><a href='https://github.com/sgrvinod/a-PyTorch-Tutorial-to-Object-Detection#evaluation'><span>Evaluation</span></a><span> stage for computing the Mean Average Precision (mAP) metric. We also have the option of filtering out </span><em><span>difficult</span></em><span> objects entirely from our data to speed up training at the cost of some accuracy.</span></p><p><span>Additionally, inside this class, </span><strong><span>each image and the objects in them are subject to a slew of transformations</span></strong><span> as described in the paper and outlined below.</span></p><h4 id='data-transforms'><span>Data Transforms</span></h4><p><span>See </span><code>transform()</code><span> in </span><a href='https://github.com/sgrvinod/a-PyTorch-Tutorial-to-Object-Detection/blob/master/utils.py'><code>utils.py</code></a><span>.</span></p><p><span>This function applies the following transformations to the images and the objects in them –</span></p><ul><li><span>Randomly </span><strong><span>adjust brightness, contrast, saturation, and hue</span></strong><span>, each with a 50% chance and in random order.</span></li><li><span>With a 50% chance, </span><strong><span>perform a </span><em><span>zoom out</span></em><span> operation</span></strong><span> on the image. This helps with learning to detect small objects. The zoomed out image must be between </span><code>1</code><span> and </span><code>4</code><span> times as large as the original. The surrounding space could be filled with the mean of the ImageNet data.</span></li><li><span>Randomly crop image, i.e. </span><strong><span>perform a </span><em><span>zoom in</span></em><span> operation.</span></strong><span> This helps with learning to detect large or partial objects. Some objects may even be cut out entirely. Crop dimensions are to be between </span><code>0.3</code><span> and </span><code>1</code><span> times the original dimensions. The aspect ratio is to be between </span><code>0.5</code><span> and </span><code>2</code><span>. Each crop is made such that there is at least one bounding box remaining that has a Jaccard overlap of either </span><code>0</code><span>, </span><code>0.1</code><span>, </span><code>0.3</code><span>, </span><code>0.5</code><span>, </span><code>0.7</code><span>, or </span><code>0.9</code><span>, randomly chosen, with the cropped image. In addition, any bounding boxes remaining whose centers are no longer in the image as a result of the crop are discarded. There is also a chance that the image is not cropped at all.</span></li><li><span>With a 50% chance, </span><strong><span>horizontally flip</span></strong><span> the image.</span></li><li><strong><span>Resize</span></strong><span> the image to </span><code>300, 300</code><span> pixels. This is a requirement of the SSD300.</span></li><li><span>Convert all boxes from </span><strong><span>absolute to fractional boundary coordinates.</span></strong><span> At all stages in our model, all boundary and center-size coordinates will be in their fractional forms.</span></li><li><strong><span>Normalize</span></strong><span> the image with the mean and standard deviation of the ImageNet data that was used to pretrain our VGG base.</span></li></ul><p><span>As mentioned in the paper, these transformations play a crucial role in obtaining the stated results.</span></p><h4 id='pytorch-dataloader'><span>PyTorch DataLoader</span></h4><p><span>The </span><code>Dataset</code><span> described above, </span><code>PascalVOCDataset</code><span>, will be used by a PyTorch </span><a href='https://pytorch.org/docs/master/data.html#torch.utils.data.DataLoader'><code>DataLoader</code></a><span> in </span><code>train.py</code><span> to </span><strong><span>create and feed batches of data to the model</span></strong><span> for training or evaluation.</span></p><p><span>Since the number of objects vary across different images, their bounding boxes, labels, and difficulties cannot simply be stacked together in the batch. There would be no way of knowing which objects belong to which image.</span></p><p><span>Instead, we need to </span><strong><span>pass a collating function to the </span><code>collate_fn</code><span> argument</span></strong><span>, which instructs the </span><code>DataLoader</code><span> about how it should combine these varying size tensors. The simplest option would be to use Python lists.</span></p><h3 id='base-convolutions'><span>Base Convolutions</span></h3><p><span>See </span><code>VGGBase</code><span> in </span><a href='https://github.com/sgrvinod/a-PyTorch-Tutorial-to-Object-Detection/blob/master/model.py'><code>model.py</code></a><span>.</span></p><p><span>Here, we </span><strong><span>create and apply base convolutions.</span></strong></p><p><span>The layers are initialized with parameters from a pretrained VGG-16 with the </span><code>load_pretrained_layers()</code><span> method.</span></p><p><span>We&#39;re especially interested in the lower-level feature maps that result from </span><code>conv4_3</code><span> and </span><code>conv7</code><span>, which we return for use in subsequent stages.</span></p><h3 id='auxiliary-convolutions-2'><span>Auxiliary Convolutions</span></h3><p><span>See </span><code>AuxiliaryConvolutions</code><span> in </span><a href='https://github.com/sgrvinod/a-PyTorch-Tutorial-to-Object-Detection/blob/master/model.py'><code>model.py</code></a><span>.</span></p><p><span>Here, we </span><strong><span>create and apply auxiliary convolutions.</span></strong></p><p><span>Use a </span><a href='https://pytorch.org/docs/stable/nn.html#torch.nn.init.xavier_uniform_'><span>uniform Xavier initialization</span></a><span> for the parameters of these layers.</span></p><p><span>We&#39;re especially interested in the higher-level feature maps that result from </span><code>conv8_2</code><span>, </span><code>conv9_2</code><span>, </span><code>conv10_2</code><span> and </span><code>conv11_2</code><span>, which we return for use in subsequent stages.</span></p><h3 id='prediction-convolutions-2'><span>Prediction Convolutions</span></h3><p><span>See </span><code>PredictionConvolutions</code><span> in </span><a href='https://github.com/sgrvinod/a-PyTorch-Tutorial-to-Object-Detection/blob/master/model.py'><code>model.py</code></a><span>.</span></p><p><span>Here, we </span><strong><span>create and apply localization and class prediction convolutions</span></strong><span> to the feature maps from </span><code>conv4_3</code><span>, </span><code>conv7</code><span>, </span><code>conv8_2</code><span>, </span><code>conv9_2</code><span>, </span><code>conv10_2</code><span> and </span><code>conv11_2</code><span>.</span></p><p><span>These layers are initialized in a manner similar to the auxiliary convolutions.</span></p><p><span>We also </span><strong><span>reshape the resulting prediction maps and stack them</span></strong><span> as discussed. Note that reshaping in PyTorch is only possible if the original tensor is stored in a </span><a href='https://pytorch.org/docs/stable/tensors.html#torch.Tensor.contiguous'><span>contiguous</span></a><span> chunk of memory.</span></p><p><span>As expected, the stacked localization and class predictions will be of dimensions </span><code>8732, 4</code><span> and </span><code>8732, 21</code><span> respectively.</span></p><h3 id='putting-it-all-together'><span>Putting it all together</span></h3><p><span>See </span><code>SSD300</code><span> in </span><a href='https://github.com/sgrvinod/a-PyTorch-Tutorial-to-Object-Detection/blob/master/model.py'><code>model.py</code></a><span>.</span></p><p><span>Here, the </span><strong><span>base, auxiliary, and prediction convolutions are combined</span></strong><span> to form the SSD.</span></p><p><span>There is a small detail here – the lowest level features, i.e. those from </span><code>conv4_3</code><span>, are expected to be on a significantly different numerical scale compared to its higher-level counterparts. Therefore, the authors recommend L2-normalizing and then rescaling </span><em><span>each</span></em><span> of its channels by a learnable value.</span></p><h3 id='priors-2'><span>Priors</span></h3><p><span>See </span><code>create_prior_boxes()</code><span> under </span><code>SSD300</code><span> in </span><a href='https://github.com/sgrvinod/a-PyTorch-Tutorial-to-Object-Detection/blob/master/model.py'><code>model.py</code></a><span>.</span></p><p><span>This function </span><strong><span>creates the priors in center-size coordinates</span></strong><span> as defined for the feature maps from </span><code>conv4_3</code><span>, </span><code>conv7</code><span>, </span><code>conv8_2</code><span>, </span><code>conv9_2</code><span>, </span><code>conv10_2</code><span> and </span><code>conv11_2</code><span>, </span><em><span>in that order</span></em><span>. Furthermore, for each feature map, we create the priors at each tile by traversing it row-wise.</span></p><p><span>This ordering of the 8732 priors thus obtained is very important because it needs to match the order of the stacked predictions.</span></p><h3 id='multibox-loss-2'><span>Multibox Loss</span></h3><p><span>See </span><code>MultiBoxLoss</code><span> in </span><a href='https://github.com/sgrvinod/a-PyTorch-Tutorial-to-Object-Detection/blob/master/model.py'><code>model.py</code></a><span>.</span></p><p><span>Two empty tensors are created to store localization and class prediction targets, i.e. </span><em><span>ground truths</span></em><span>, for the 8732 predicted boxes in each image.</span></p><p><span>We </span><strong><span>find the ground truth object with the maximum Jaccard overlap for each prior</span></strong><span>, which is stored in </span><code>object_for_each_prior</code><span>.</span></p><p><span>We want to avoid the rare situation where not all of the ground truth objects have been matched. Therefore, we also </span><strong><span>find the prior with the maximum overlap for each ground truth object</span></strong><span>, stored in </span><code>prior_for_each_object</code><span>. We explicitly add these matches to </span><code>object_for_each_prior</code><span> and artificially set their overlaps to a value above the threshold so they are not eliminated.</span></p><p><span>Based on the matches in </span><code>object_for_each prior</code><span>, we set the corresponding labels, i.e. </span><strong><span>targets for class prediction</span></strong><span>, to each of the 8732 priors. For those priors that don&#39;t overlap significantly with their matched objects, the label is set to </span><em><span>background</span></em><span>.</span></p><p><span>Also, we encode the coordinates of the 8732 matched objects in </span><code>object_for_each prior</code><span> in offset form </span><code>(g_c_x, g_c_y, g_w, g_h)</code><span> with respect to these priors, to form the </span><strong><span>targets for localization</span></strong><span>. Not all of these 8732 localization targets are meaningful. As we discussed earlier, only the predictions arising from the non-background priors will be regressed to their targets.</span></p><p><span>The </span><strong><span>localization loss</span></strong><span> is the </span><a href='https://pytorch.org/docs/stable/nn.html#torch.nn.SmoothL1Loss'><span>Smooth L1 loss</span></a><span> over the positive matches.</span></p><p><span>Perform Hard Negative Mining – rank class predictions matched to </span><em><span>background</span></em><span>, i.e. negative matches, by their individual </span><a href='https://pytorch.org/docs/stable/nn.html#torch.nn.CrossEntropyLoss'><span>Cross Entropy losses</span></a><span>. The </span><strong><span>confidence loss</span></strong><span> is the Cross Entropy loss over the positive matches and the hardest negative matches. Nevertheless, it is averaged only by the number of positive matches.</span></p><p><span>The </span><strong><span>Multibox Loss is the aggregate of these two losses</span></strong><span>, combined in the ratio </span><code>α</code><span>. In our case, they are simply being added because </span><code>α = 1</code><span>.</span></p><h1 id='training'><span>Training</span></h1><p><span>Before you begin, make sure to save the required data files for training and evaluation. To do this, run the contents of </span><a href='https://github.com/sgrvinod/a-PyTorch-Tutorial-to-Object-Detection/blob/master/create_data_lists.py'><code>create_data_lists.py</code></a><span> after pointing it to the </span><code>VOC2007</code><span> and </span><code>VOC2012</code><span> folders in your </span><a href='https://github.com/sgrvinod/a-PyTorch-Tutorial-to-Object-Detection#download'><span>downloaded data</span></a><span>.</span></p><p><span>See </span><a href='https://github.com/sgrvinod/a-PyTorch-Tutorial-to-Object-Detection/blob/master/train.py'><code>train.py</code></a><span>.</span></p><p><span>The parameters for the model (and training it) are at the beginning of the file, so you can easily check or modify them should you need to.</span></p><p><span>To </span><strong><span>train your model from scratch</span></strong><span>, run this file –</span></p><p><code>python train.py</code></p><p><span>To </span><strong><span>resume training at a checkpoint</span></strong><span>, point to the corresponding file with the </span><code>checkpoint</code><span> parameter at the beginning of the code.</span></p><h3 id='remarks'><span>Remarks</span></h3><p><span>In the paper, they recommend using </span><strong><span>Stochastic Gradient Descent</span></strong><span> in batches of </span><code>32</code><span> images, with an initial learning rate of </span><code>1e−3</code><span>, momentum of </span><code>0.9</code><span>, and </span><code>5e-4</code><span> weight decay.</span></p><p><span>I ended up using a batch size of </span><code>8</code><span> images for increased stability. If you find that your gradients are exploding, you could reduce the batch size, like I did, or clip gradients.</span></p><p><span>The authors also doubled the learning rate for bias parameters. As you can see in the code, this is easy do in PyTorch, by passing </span><a href='https://pytorch.org/docs/stable/optim.html#per-parameter-options'><span>separate groups of parameters</span></a><span> to the </span><code>params</code><span> argument of its </span><a href='https://pytorch.org/docs/stable/optim.html#torch.optim.SGD'><span>SGD optimizer</span></a><span>.</span></p><p><span>The paper recommends training for 80000 iterations at the initial learning rate. Then, it is decayed by 90% (i.e. to a tenth) for an additional 20000 iterations, </span><em><span>twice</span></em><span>. With the paper&#39;s batch size of </span><code>32</code><span>, this means that the learning rate is decayed by 90% once after the 154th epoch and once more after the 193th epoch, and training is stopped after 232 epochs. I followed this schedule.</span></p><p><span>On a TitanX (Pascal), each epoch of training required about 6 minutes.</span></p><h3 id='model-checkpoint'><span>Model checkpoint</span></h3><p><span>You can download this pretrained model </span><a href='https://drive.google.com/open?id=1bvJfF6r_zYl2xZEpYXxgb7jLQHFZ01Qe'><span>here</span></a><span>.</span></p><p><span>Note that this checkpoint should be </span><a href='https://pytorch.org/docs/stable/torch.html?#torch.load'><span>loaded directly with PyTorch</span></a><span> for evaluation or inference – see below.</span></p><h1 id='evaluation'><span>Evaluation</span></h1><p><span>See </span><a href='https://github.com/sgrvinod/a-PyTorch-Tutorial-to-Object-Detection/blob/master/eval.py'><code>eval.py</code></a><span>.</span></p><p><span>The data-loading and checkpoint parameters for evaluating the model are at the beginning of the file, so you can easily check or modify them should you wish to.</span></p><p><span>To begin evaluation, simply run the </span><code>evaluate()</code><span> function with the data-loader and model checkpoint. </span><strong><span>Raw predictions for each image in the test set are obtained and parsed</span></strong><span> with the checkpoint&#39;s </span><code>detect_objects()</code><span> method, which implements </span><a href='https://github.com/sgrvinod/a-PyTorch-Tutorial-to-Object-Detection#processing-predictions'><span>this process</span></a><span>. Evaluation has to be done at a </span><code>min_score</code><span> of </span><code>0.01</code><span>, an NMS </span><code>max_overlap</code><span> of </span><code>0.45</code><span>, and </span><code>top_k</code><span> of </span><code>200</code><span> to allow fair comparision of results with the paper and other implementations.</span></p><p><strong><span>Parsed predictions are evaluated against the ground truth objects.</span></strong><span> The evaluation metric is the </span><em><span>Mean Average Precision (mAP)</span></em><span>. If you&#39;re not familiar with this metric, </span><a href='https://medium.com/@jonathan_hui/map-mean-average-precision-for-object-detection-45c121a31173'><span>here&#39;s a great explanation</span></a><span>.</span></p><p><span>We will use </span><code>calculate_mAP()</code><span> in </span><a href='https://github.com/sgrvinod/a-PyTorch-Tutorial-to-Object-Detection/blob/master/utils.py'><code>utils.py</code></a><span> for this purpose. As is the norm, we will ignore </span><em><span>difficult</span></em><span> detections in the mAP calculation. But nevertheless, it is important to include them from the evaluation dataset because if the model does detect an object that is considered to be </span><em><span>difficult</span></em><span>, it must not be counted as a false positive.</span></p><p><span>The model scores </span><strong><span>77.2 mAP</span></strong><span>, same as the result reported in the paper.</span></p><p><span>Class-wise average precisions (not scaled to 100) are listed below.</span></p><figure><table><thead><tr><th style='text-align:center;' ><span>Class</span></th><th style='text-align:center;' ><span>Average Precision</span></th></tr></thead><tbody><tr><td style='text-align:center;' ><em><span>aeroplane</span></em></td><td style='text-align:center;' ><span>0.7887580990791321</span></td></tr><tr><td style='text-align:center;' ><em><span>bicycle</span></em></td><td style='text-align:center;' ><span>0.8351995348930359</span></td></tr><tr><td style='text-align:center;' ><em><span>bird</span></em></td><td style='text-align:center;' ><span>0.7623348236083984</span></td></tr><tr><td style='text-align:center;' ><em><span>boat</span></em></td><td style='text-align:center;' ><span>0.7218425273895264</span></td></tr><tr><td style='text-align:center;' ><em><span>bottle</span></em></td><td style='text-align:center;' ><span>0.45978495478630066</span></td></tr><tr><td style='text-align:center;' ><em><span>bus</span></em></td><td style='text-align:center;' ><span>0.8705356121063232</span></td></tr><tr><td style='text-align:center;' ><em><span>car</span></em></td><td style='text-align:center;' ><span>0.8655831217765808</span></td></tr><tr><td style='text-align:center;' ><em><span>cat</span></em></td><td style='text-align:center;' ><span>0.8828985095024109</span></td></tr><tr><td style='text-align:center;' ><em><span>chair</span></em></td><td style='text-align:center;' ><span>0.5917483568191528</span></td></tr><tr><td style='text-align:center;' ><em><span>cow</span></em></td><td style='text-align:center;' ><span>0.8255912661552429</span></td></tr><tr><td style='text-align:center;' ><em><span>diningtable</span></em></td><td style='text-align:center;' ><span>0.756867527961731</span></td></tr><tr><td style='text-align:center;' ><em><span>dog</span></em></td><td style='text-align:center;' ><span>0.856262743473053</span></td></tr><tr><td style='text-align:center;' ><em><span>horse</span></em></td><td style='text-align:center;' ><span>0.8778411149978638</span></td></tr><tr><td style='text-align:center;' ><em><span>motorbike</span></em></td><td style='text-align:center;' ><span>0.8316892385482788</span></td></tr><tr><td style='text-align:center;' ><em><span>person</span></em></td><td style='text-align:center;' ><span>0.7884440422058105</span></td></tr><tr><td style='text-align:center;' ><em><span>pottedplant</span></em></td><td style='text-align:center;' ><span>0.5071538090705872</span></td></tr><tr><td style='text-align:center;' ><em><span>sheep</span></em></td><td style='text-align:center;' ><span>0.7936667799949646</span></td></tr><tr><td style='text-align:center;' ><em><span>sofa</span></em></td><td style='text-align:center;' ><span>0.7998116612434387</span></td></tr><tr><td style='text-align:center;' ><em><span>train</span></em></td><td style='text-align:center;' ><span>0.8655905723571777</span></td></tr><tr><td style='text-align:center;' ><em><span>tvmonitor</span></em></td><td style='text-align:center;' ><span>0.7492395043373108</span></td></tr></tbody></table></figure><p><span>You can see that some objects, like bottles and potted plants, are considerably harder to detect than others.</span></p><h1 id='inference'><span>Inference</span></h1><p><span>See </span><a href='https://github.com/sgrvinod/a-PyTorch-Tutorial-to-Object-Detection/blob/master/detect.py'><code>detect.py</code></a><span>.</span></p><p><span>Point to the model you want to use for inference with the </span><code>checkpoint</code><span> parameter at the beginning of the code.</span></p><p><span>Then, you can use the </span><code>detect()</code><span> function to identify and visualize objects in an RGB image.</span></p><pre class="md-fences md-end-block ty-contain-cm modeLoaded" spellcheck="false" lang="python"><div class="CodeMirror cm-s-inner cm-s-null-scroll CodeMirror-wrap" lang="python"><div style="overflow: hidden; position: relative; width: 3px; height: 0px; top: 9.5px; left: 8px;"><textarea autocorrect="off" autocapitalize="off" spellcheck="false" tabindex="0" style="position: absolute; bottom: -1em; padding: 0px; width: 1000px; height: 1em; outline: none;"></textarea></div><div class="CodeMirror-scrollbar-filler" cm-not-content="true"></div><div class="CodeMirror-gutter-filler" cm-not-content="true"></div><div class="CodeMirror-scroll" tabindex="-1"><div class="CodeMirror-sizer" style="margin-left: 0px; margin-bottom: 0px; border-right-width: 0px; padding-right: 0px; padding-bottom: 0px;"><div style="position: relative; top: 0px;"><div class="CodeMirror-lines" role="presentation"><div role="presentation" style="position: relative; outline: none;"><div class="CodeMirror-measure"><pre><span>xxxxxxxxxx</span></pre></div><div class="CodeMirror-measure"></div><div style="position: relative; z-index: 1;"></div><div class="CodeMirror-code" role="presentation" style=""><div class="CodeMirror-activeline" style="position: relative;"><div class="CodeMirror-activeline-background CodeMirror-linebackground"></div><div class="CodeMirror-gutter-background CodeMirror-activeline-gutter" style="left: 0px; width: 0px;"></div><pre class=" CodeMirror-line " role="presentation"><span role="presentation" style="padding-right: 0.1px;"><span class="cm-variable">img_path</span> = <span class="cm-string">'/path/to/ima.ge'</span></span></pre></div><pre class=" CodeMirror-line " role="presentation"><span role="presentation" style="padding-right: 0.1px;"><span class="cm-variable">original_image</span> = <span class="cm-variable">PIL</span>.<span class="cm-property">Image</span>.<span class="cm-property">open</span>(<span class="cm-variable">img_path</span>, <span class="cm-variable">mode</span>=<span class="cm-string">'r'</span>)</span></pre><pre class=" CodeMirror-line " role="presentation"><span role="presentation" style="padding-right: 0.1px;"><span class="cm-variable">original_image</span> = <span class="cm-variable">original_image</span>.<span class="cm-property">convert</span>(<span class="cm-string">'RGB'</span>)</span></pre><pre class=" CodeMirror-line " role="presentation"><span role="presentation" style="padding-right: 0.1px;"><span cm-text="" cm-zwsp="">
</span></span></pre><pre class=" CodeMirror-line " role="presentation"><span role="presentation" style="padding-right: 0.1px;"><span class="cm-variable">detect</span>(<span class="cm-variable">original_image</span>, <span class="cm-variable">min_score</span>=<span class="cm-number">0.2</span>, <span class="cm-variable">max_overlap</span>=<span class="cm-number">0.5</span>, <span class="cm-variable">top_k</span>=<span class="cm-number">200</span>).<span class="cm-property">show</span>()</span></pre></div></div></div></div></div><div style="position: absolute; height: 0px; width: 1px; border-bottom: 0px solid transparent; top: 115px;"></div><div class="CodeMirror-gutters" style="display: none; height: 115px;"></div></div></div></pre><p><span>This function first </span><strong><span>preprocesses the image by resizing and normalizing its RGB channels</span></strong><span> as required by the model. It then </span><strong><span>obtains raw predictions from the model, which are parsed</span></strong><span> by the </span><code>detect_objects()</code><span> method in the model. The parsed results are converted from fractional to absolute boundary coordinates, their labels are decoded with the </span><code>label_map</code><span>, and they are </span><strong><span>visualized on the image</span></strong><span>.</span></p><p><span>There are no one-size-fits-all values for </span><code>min_score</code><span>, </span><code>max_overlap</code><span>, and </span><code>top_k</code><span>. You may need to experiment a little to find what works best for your target data.</span></p><h3 id='some-more-examples'><span>Some more examples</span></h3><hr /><p align="center">
<img src="./img/000029.jpg">
</p><hr /><p align="center">
<img src="./img/000045.jpg">
</p><hr /><p align="center">
<img src="./img/000062.jpg">
</p><hr /><p align="center">
<img src="./img/000075.jpg">
</p><hr /><p align="center">
<img src="./img/000085.jpg">
</p><hr /><p align="center">
<img src="./img/000092.jpg">
</p><hr /><p align="center">
<img src="./img/000100.jpg">
</p><hr /><p align="center">
<img src="./img/000124.jpg">
</p><hr /><p align="center">
<img src="./img/000127.jpg">
</p><hr /><p align="center">
<img src="./img/000128.jpg">
</p><hr /><p align="center">
<img src="./img/000145.jpg">
</p><hr /><h1 id='faqs'><span>FAQs</span></h1><p><strong><span>I noticed that priors often overshoot the </span><code>3, 3</code><span> kernel employed in the prediction convolutions. How can the kernel detect a bound (of an object) outside it?</span></strong></p><p><span>Don&#39;t confuse the kernel and its </span><em><span>receptive field</span></em><span>, which is the area of the original image that is represented in the kernel&#39;s field-of-view.</span></p><p><span>For example, on the </span><code>38, 38</code><span> feature map from </span><code>conv4_3</code><span>, a </span><code>3, 3</code><span> kernel covers an area of </span><code>0.08, 0.08</code><span> in fractional coordinates. The priors are </span><code>0.1, 0.1</code><span>, </span><code>0.14, 0.07</code><span>, </span><code>0.07, 0.14</code><span>, and </span><code>0.14, 0.14</code><span>.</span></p><p><span>But its receptive field, which </span><a href='https://medium.com/mlreview/a-guide-to-receptive-field-arithmetic-for-convolutional-neural-networks-e0f514068807'><span>you can calculate</span></a><span>, is a whopping </span><code>0.36, 0.36</code><span>! Therefore, all priors (and objects contained therein) are present well inside it.</span></p><p><span>Keep in mind that the receptive field grows with every successive convolution. For </span><code>conv_7</code><span> and the higher-level feature maps, a </span><code>3, 3</code><span> kernel&#39;s receptive field will cover the </span><em><span>entire</span></em><span> </span><code>300, 300</code><span> image. But, as always, the pixels in the original image that are closer to the center of the kernel have greater representation, so it is still </span><em><span>local</span></em><span> in a sense.</span></p><hr /><p><strong><span>While training, why can&#39;t we match predicted boxes directly to their ground truths?</span></strong></p><p><span>We cannot directly check for overlap or coincidence between predicted boxes and ground truth objects to match them because predicted boxes are not to be considered reliable, </span><em><span>especially</span></em><span> during the training process. This is the very reason we are trying to evaluate them in the first place!</span></p><p><span>And this is why priors are especially useful. We can match a predicted box to a ground truth box by means of the prior it is supposed to be approximating. It no longer matters how correct or wildly wrong the prediction is.</span></p><hr /><p><strong><span>Why do we even have a </span><em><span>background</span></em><span> class if we&#39;re only checking which </span><em><span>non-background</span></em><span> classes meet the threshold?</span></strong></p><p><span>When there is no object in the approximate field of the prior, a high score for </span><em><span>background</span></em><span> will dilute the scores of the other classes such that they will not meet the detection threshold.</span></p><hr /><p><strong><span>Why not simply choose the class with the highest score instead of using a threshold?</span></strong></p><p><span>I think that&#39;s a valid strategy. After all, we implicitly conditioned the model to choose </span><em><span>one</span></em><span> class when we trained it with the Cross Entropy loss. But you will find that you won&#39;t achieve the same performance as you would with a threshold.</span></p><p><span>I suspect this is because object detection is open-ended enough that there&#39;s room for doubt in the trained model as to what&#39;s really in the field of the prior. For example, the score for </span><em><span>background</span></em><span> may be high if there is an appreciable amount of backdrop visible in an object&#39;s bounding box. There may even be multiple objects present in the same approximate region. A simple threshold will yield all possibilities for our consideration, and it just works better.</span></p><p><span>Redundant detections aren&#39;t really a problem since we&#39;re NMS-ing the hell out of &#39;em.</span></p><hr /><p><strong><span>Sorry, but I gotta ask... </span><em><a href='https://cnet4.cbsistatic.com/img/cLD5YVGT9pFqx61TuMtcSBtDPyY=/570x0/2017/01/14/6d8103f7-a52d-46de-98d0-56d0e9d79804/se7en.png'><span>what&#39;s in the boooox?!</span></a></em></strong></p><p><span>Ha.</span></p></div></div>
</body>
</html>